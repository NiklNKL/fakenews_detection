{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nikl/programming/fhdw/knowledge_engineering_ausarbeitung/fakenews_detection/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from custom_utils import load_and_concatenate_parquet_files\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import AutoTokenizer, AutoConfig, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "from peft import get_peft_model, LoraConfig\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>label_names</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>donald trump respond mockery fake swedish atta...</td>\n",
       "      <td>1</td>\n",
       "      <td>real</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>tweetwavethis time true pantstweetwave anthony...</td>\n",
       "      <td>1</td>\n",
       "      <td>real</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>rubio prospect trump president worrisome reute...</td>\n",
       "      <td>0</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>trump lifts cyber command status boost cyber d...</td>\n",
       "      <td>0</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>big republican lie economy tear apart minute v...</td>\n",
       "      <td>1</td>\n",
       "      <td>real</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63116</th>\n",
       "      <td>half briton want stay eu polledinburgh reuters...</td>\n",
       "      <td>0</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63117</th>\n",
       "      <td>bill hillary clinton inc sale right pricein sp...</td>\n",
       "      <td>1</td>\n",
       "      <td>real</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63118</th>\n",
       "      <td>orlando gunman shoot time autopsy find new yor...</td>\n",
       "      <td>0</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63119</th>\n",
       "      <td>lethal gap supreme court handle death penalty ...</td>\n",
       "      <td>0</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63120</th>\n",
       "      <td>poll world overwhelmingly love president obama...</td>\n",
       "      <td>1</td>\n",
       "      <td>real</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>63121 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  label label_names\n",
       "0      donald trump respond mockery fake swedish atta...      1        real\n",
       "1      tweetwavethis time true pantstweetwave anthony...      1        real\n",
       "2      rubio prospect trump president worrisome reute...      0        fake\n",
       "3      trump lifts cyber command status boost cyber d...      0        fake\n",
       "4      big republican lie economy tear apart minute v...      1        real\n",
       "...                                                  ...    ...         ...\n",
       "63116  half briton want stay eu polledinburgh reuters...      0        fake\n",
       "63117  bill hillary clinton inc sale right pricein sp...      1        real\n",
       "63118  orlando gunman shoot time autopsy find new yor...      0        fake\n",
       "63119  lethal gap supreme court handle death penalty ...      0        fake\n",
       "63120  poll world overwhelmingly love president obama...      1        real\n",
       "\n",
       "[63121 rows x 3 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df  = load_and_concatenate_parquet_files('data/preprocessed_big_training_df')\n",
    "\n",
    "df = df.rename(columns={'preprocessed_text': 'text'})\n",
    "df[\"label_names\"] = df[\"label\"].apply(lambda x: \"real\" if x == 1 else \"fake\")\n",
    "display(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((44184, 3), (12624, 3), (6313, 3))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train,test      = train_test_split(df,test_size=0.3,stratify=df['label'])\n",
    "test,validation = train_test_split(test,test_size=1/3,stratify=test['label'])\n",
    "train.shape, test.shape, validation.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label', 'label_names'],\n",
       "        num_rows: 44184\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label', 'label_names'],\n",
       "        num_rows: 12624\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text', 'label', 'label_names'],\n",
       "        num_rows: 6313\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = DatasetDict(\n",
    "    {'train':Dataset.from_pandas(train,preserve_index=False),\n",
    "     'test':Dataset.from_pandas(test,preserve_index=False),\n",
    "     'validation': Dataset.from_pandas(validation,preserve_index=False)\n",
    "     }    \n",
    ")\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'real': 1, 'fake': 0}, {1: 'real', 0: 'fake'})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label2id = {x['label_names']:x['label'] for x in dataset['train']}\n",
    "id2label = {v:k for k,v in label2id.items()}\n",
    "label2id, id2label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DebertaV2Tokenizer\n",
    "# Load dataset\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load the evaluation metric\n",
    "accuracy_metric = evaluate.load(\"accuracy\")\n",
    "f1_metric = evaluate.load(\"f1\")\n",
    "precision_metric = evaluate.load(\"precision\")\n",
    "recall_metric = evaluate.load(\"recall\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ckpt = \"bert-base-uncased\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_format(batch):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_ckpt)   \n",
    "    tokens = tokenizer(batch['text'], padding=True, truncation=True)\n",
    "    # Convert to PyTorch tensors and move to the correct device\n",
    "    tokens = {key: torch.tensor(val).to(device) for key, val in tokens.items()}\n",
    "    tokens['labels'] = torch.tensor(batch['label']).to(device)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 44184/44184 [00:24<00:00, 1833.17 examples/s]\n",
      "Map: 100%|██████████| 12624/12624 [00:06<00:00, 2041.42 examples/s]\n",
      "Map: 100%|██████████| 6313/6313 [00:03<00:00, 1941.68 examples/s]\n"
     ]
    }
   ],
   "source": [
    "tokenized_dataset = dataset.map(tokenize_and_format, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "config = AutoConfig.from_pretrained(model_ckpt, num_labels=2)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"/home/nikl/programming/fhdw/knowledge_engineering_ausarbeitung/fakenews_detection/models/roberta-base/checkpoint-6905\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['text', 'label', 'label_names', 'input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 12624\n",
      "})\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "string indices must be integers",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 40\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m all_labels, all_preds\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m# Run evaluation\u001b[39;00m\n\u001b[0;32m---> 40\u001b[0m all_labels, all_preds \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenized_dataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtest\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m# Compute metrics\u001b[39;00m\n\u001b[1;32m     43\u001b[0m accuracy \u001b[38;5;241m=\u001b[39m accuracy_score(all_labels, all_preds)\n",
      "Cell \u001b[0;32mIn[15], line 23\u001b[0m, in \u001b[0;36mevaluate_model\u001b[0;34m(model, dataset, device, batch_size)\u001b[0m\n\u001b[1;32m     20\u001b[0m batch \u001b[38;5;241m=\u001b[39m dataset[i:i \u001b[38;5;241m+\u001b[39m batch_size]\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# Extract inputs and labels\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m input_ids \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack([example[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m example \u001b[38;5;129;01min\u001b[39;00m batch])\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     24\u001b[0m attention_mask \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack([example[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m example \u001b[38;5;129;01min\u001b[39;00m batch])\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     25\u001b[0m labels \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([example[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m example \u001b[38;5;129;01min\u001b[39;00m batch])\u001b[38;5;241m.\u001b[39mto(device)\n",
      "Cell \u001b[0;32mIn[15], line 23\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     20\u001b[0m batch \u001b[38;5;241m=\u001b[39m dataset[i:i \u001b[38;5;241m+\u001b[39m batch_size]\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# Extract inputs and labels\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m input_ids \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack([\u001b[43mexample\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43minput_ids\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m example \u001b[38;5;129;01min\u001b[39;00m batch])\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     24\u001b[0m attention_mask \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack([example[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m example \u001b[38;5;129;01min\u001b[39;00m batch])\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     25\u001b[0m labels \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([example[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m example \u001b[38;5;129;01min\u001b[39;00m batch])\u001b[38;5;241m.\u001b[39mto(device)\n",
      "\u001b[0;31mTypeError\u001b[0m: string indices must be integers"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoConfig, AutoModelForSequenceClassification\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# def collate_fn(batch):\n",
    "#     return {key: torch.stack([example[key] for example in batch]) for key in batch[0]}\n",
    "# Assuming `test_set` is your tokenized dataset\n",
    "\n",
    "print(tokenized_dataset[\"test\"])\n",
    "# test_loader = DataLoader(tokenized_dataset[\"test\"], batch_size=32, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "def evaluate_model(model, dataset, device, batch_size=32):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    # Loop through the dataset in batches\n",
    "    for i in range(0, len(dataset), batch_size):\n",
    "        batch = dataset[i:i + batch_size]\n",
    "        \n",
    "        # Extract inputs and labels\n",
    "        input_ids = torch.stack([example['input_ids'] for example in batch]).to(device)\n",
    "        attention_mask = torch.stack([example['attention_mask'] for example in batch]).to(device)\n",
    "        labels = torch.tensor([example['labels'] for example in batch]).to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            logits = outputs.logits\n",
    "            preds = torch.argmax(logits, dim=1)  # Get predicted labels\n",
    "            \n",
    "        # Append predictions and labels to lists\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    return all_labels, all_preds\n",
    "\n",
    "\n",
    "# Run evaluation\n",
    "all_labels, all_preds = evaluate_model(model, tokenized_dataset[\"test\"], device)\n",
    "\n",
    "# Compute metrics\n",
    "accuracy = accuracy_score(all_labels, all_preds)\n",
    "precision = precision_score(all_labels, all_preds, average='binary')\n",
    "recall = recall_score(all_labels, all_preds, average='binary')\n",
    "f1 = f1_score(all_labels, all_preds, average='binary')\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "\n",
    "# Optional: Display full classification report\n",
    "print(\"\\nClassification Report:\\n\", classification_report(all_labels, all_preds))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, precision_score, recall_score, f1_score\n",
    "from custom_utils import load_and_concatenate_parquet_files\n",
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from peft import get_peft_model, LoraConfig\n",
    "import torch\n",
    "\n",
    "# Define the directory where model results are stored\n",
    "MODEL_RESULTS_DIR = \"models\"\n",
    "\n",
    "# List of model checkpoints used in training\n",
    "model_checkpoints = [\n",
    "    \"bert-base-uncased\",\n",
    "    \"distilbert-base-uncased\",\n",
    "    \"roberta-base\",\n",
    "    # \"microsoft/deberta-v3-base\"\n",
    "]\n",
    "\n",
    "# Load the test dataset\n",
    "df = load_and_concatenate_parquet_files('data/preprocessed_big_training_df')\n",
    "df = df.rename(columns={'preprocessed_text': 'text'})\n",
    "df[\"label_names\"] = df[\"label\"].apply(lambda x: \"real\" if x == 1 else \"fake\")\n",
    "\n",
    "# Split the dataset (ensure test split matches training)\n",
    "_, test = train_test_split(df, test_size=0.3, stratify=df['label'])\n",
    "_, test = train_test_split(test, test_size=1/3, stratify=test['label'])\n",
    "\n",
    "test_dataset = Dataset.from_pandas(test, preserve_index=False)\n",
    "\n",
    "# Function to tokenize the test dataset\n",
    "def tokenize_and_format(batch, model, device):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model) \n",
    "    tokens = tokenizer(batch['text'], padding=True, truncation=True)\n",
    "    # Convert to PyTorch tensors and move to the correct device\n",
    "    tokens = {key: torch.tensor(val).to(device) for key, val in tokens.items()}\n",
    "    tokens['labels'] = torch.tensor(batch['label']).to(device)\n",
    "    return tokens\n",
    "\n",
    "# Initialize a DataFrame to store model metrics\n",
    "results_df = pd.DataFrame(columns=[\n",
    "    \"Model\", \"Accuracy\", \"F1\", \"Precision\", \"Recall\", \"AUROC\", \"Train Loss\", \"Validation Loss\"\n",
    "])\n",
    "\n",
    "# Function to find the latest checkpoint directory\n",
    "def get_latest_checkpoint_dir(model_dir):\n",
    "    checkpoint_dirs = [d for d in os.listdir(model_dir) if d.startswith(\"checkpoint-\")]\n",
    "    if not checkpoint_dirs:\n",
    "        return None\n",
    "    latest_checkpoint = max(checkpoint_dirs, key=lambda x: int(x.split(\"-\")[-1]))\n",
    "    return os.path.join(model_dir, latest_checkpoint)\n",
    "\n",
    "# Function to load metrics and evaluate the test dataset\n",
    "def load_metrics_and_evaluate_test(model_dir, model_ckpt, test_dataset):\n",
    "    latest_checkpoint = get_latest_checkpoint_dir(model_dir)\n",
    "    if not latest_checkpoint:\n",
    "        print(f\"No checkpoints found in {model_dir}\")\n",
    "        return None\n",
    "\n",
    "    # Load model and tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n",
    "    config = AutoConfig.from_pretrained(model_ckpt, num_labels=2)\n",
    "    base_model = AutoModelForSequenceClassification.from_pretrained(model_ckpt, config=config)\n",
    "\n",
    "    peft_config = LoraConfig(\n",
    "        task_type=\"SEQ_CLS\",\n",
    "        r=8,\n",
    "        lora_alpha=32,\n",
    "        lora_dropout=0.1\n",
    "    )\n",
    "    model = get_peft_model(base_model, peft_config)\n",
    "    # model.load_state_dict(torch.load(latest_checkpoint+\"/rng_state.pth\"), strict=False)\n",
    "    model.load_adapter(os.path.join(latest_checkpoint), config=peft_config, adapter_name=\"lora\")\n",
    "   \n",
    "    model.to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "\n",
    "    # Tokenize test dataset\n",
    "    tokenized_test = test_dataset.map(lambda batch: tokenize_and_format(batch, model_ckpt, model.device), batched=True)\n",
    "    tokenized_test.set_format(\"torch\")\n",
    "    tokenized_test = tokenized_test.with_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "\n",
    "    # Evaluate on the test dataset\n",
    "    model.eval()\n",
    "    y_true, y_pred, y_pred_proba = [], [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in tokenized_test:\n",
    "            inputs = {key: val.to(model.device) for key, val in batch.items() if key != \"labels\"}\n",
    "            print(inputs)\n",
    "            outputs = model(**inputs)\n",
    "            logits = outputs.logits\n",
    "            probabilities = torch.softmax(logits, dim=-1)\n",
    "            y_pred.extend(torch.argmax(logits, dim=-1).tolist())\n",
    "            y_pred_proba.extend(probabilities[:, 1].tolist())\n",
    "            y_true.extend(batch[\"labels\"].tolist())\n",
    "\n",
    "    # Compute metrics dynamically\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred)\n",
    "    recall = recall_score(y_true, y_pred)\n",
    "    auroc = roc_auc_score(y_true, y_pred_proba)\n",
    "\n",
    "    # Load training logs\n",
    "    log_file = os.path.join(model_dir, \"trainer_state.json\")\n",
    "    if os.path.exists(log_file):\n",
    "        with open(log_file, \"r\") as f:\n",
    "            trainer_state = json.load(f)\n",
    "            train_loss = trainer_state[\"log_history\"][-1].get(\"loss\", None)  # Final training loss\n",
    "            valid_loss = trainer_state[\"log_history\"][-1].get(\"eval_loss\", None)  # Final validation loss\n",
    "    else:\n",
    "        train_loss, valid_loss = None, None\n",
    "\n",
    "    return {\n",
    "        \"accuracy\": accuracy,\n",
    "        \"f1\": f1,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"auroc\": auroc,\n",
    "        \"train_loss\": train_loss,\n",
    "        \"valid_loss\": valid_loss\n",
    "    }\n",
    "\n",
    "# Iterate over each model directory and collect metrics\n",
    "for model_ckpt in model_checkpoints:\n",
    "    model_dir = os.path.join(MODEL_RESULTS_DIR, model_ckpt.replace(\"/\", \"_\"))\n",
    "    metrics = load_metrics_and_evaluate_test(model_dir, model_ckpt, test_dataset)\n",
    "\n",
    "    if metrics:\n",
    "        results_df = results_df.append({\n",
    "            \"Model\": model_ckpt,\n",
    "            \"Accuracy\": metrics[\"accuracy\"],\n",
    "            \"F1\": metrics[\"f1\"],\n",
    "            \"Precision\": metrics[\"precision\"],\n",
    "            \"Recall\": metrics[\"recall\"],\n",
    "            \"AUROC\": metrics[\"auroc\"],\n",
    "            \"Train Loss\": metrics[\"train_loss\"],\n",
    "            \"Validation Loss\": metrics[\"valid_loss\"]\n",
    "        }, ignore_index=True)\n",
    "\n",
    "# Save results as a CSV\n",
    "results_csv_path = os.path.join(MODEL_RESULTS_DIR, \"model_comparison.csv\")\n",
    "results_df.to_csv(results_csv_path, index=False)\n",
    "\n",
    "# Display the results table\n",
    "print(results_df)\n",
    "\n",
    "# Visualization of metrics\n",
    "metrics_to_plot = [\"Accuracy\", \"F1\", \"Precision\", \"Recall\", \"AUROC\"]\n",
    "fig, axs = plt.subplots(1, len(metrics_to_plot), figsize=(20, 5))\n",
    "\n",
    "for i, metric in enumerate(metrics_to_plot):\n",
    "    axs[i].bar(results_df[\"Model\"], results_df[metric], color=\"skyblue\")\n",
    "    axs[i].set_title(metric)\n",
    "    axs[i].set_xticklabels(results_df[\"Model\"], rotation=45, ha=\"right\")\n",
    "    axs[i].set_ylabel(metric)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot loss curves\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.bar(results_df[\"Model\"], results_df[\"Train Loss\"], label=\"Train Loss\", alpha=0.7, color=\"blue\")\n",
    "plt.bar(results_df[\"Model\"], results_df[\"Validation Loss\"], label=\"Validation Loss\", alpha=0.7, color=\"orange\")\n",
    "plt.title(\"Train vs Validation Loss\")\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
