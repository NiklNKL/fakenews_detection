{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Infrastructure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data into a dataset dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['preprocessed_text', 'label', 'label_names'],\n",
       "        num_rows: 44153\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['preprocessed_text', 'label', 'label_names'],\n",
       "        num_rows: 12616\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['preprocessed_text', 'label', 'label_names'],\n",
       "        num_rows: 6308\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "train = pd.read_parquet(\"/home/nikl/programming/fhdw/knowledge_engineering_ausarbeitung/fakenews_detection/data/model_training/train_df.parquet\")\n",
    "valid = pd.read_parquet(\"/home/nikl/programming/fhdw/knowledge_engineering_ausarbeitung/fakenews_detection/data/model_training/valid_df.parquet\")\n",
    "test = pd.read_parquet(\"/home/nikl/programming/fhdw/knowledge_engineering_ausarbeitung/fakenews_detection/data/model_training/test_df.parquet\")\n",
    "\n",
    "dataset = DatasetDict(\n",
    "    {'train':Dataset.from_pandas(train,preserve_index=False),\n",
    "     'validation': Dataset.from_pandas(valid,preserve_index=False),\n",
    "     'test':Dataset.from_pandas(test,preserve_index=False)\n",
    "     }    \n",
    ")\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method for evaluating performance while training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import evaluate\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "accuracy_metric = evaluate.load(\"accuracy\")\n",
    "f1_metric = evaluate.load(\"f1\")\n",
    "precision_metric = evaluate.load(\"precision\")\n",
    "recall_metric = evaluate.load(\"recall\")\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    logits, labels = pred\n",
    "    predictions = torch.argmax(torch.tensor(logits), dim=-1)\n",
    "    result = {}\n",
    "    for metric in [accuracy_metric, f1_metric, precision_metric, recall_metric]:\n",
    "        result.update(metric.compute(predictions=predictions, references=labels))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def load_dataset(path):\n",
    "    parquet_files = glob.glob(f\"{path}/*.parquet\")\n",
    "    dataframes = {file.split('/')[-1].replace('.parquet', ''): pd.read_parquet(file) for file in parquet_files}\n",
    "    \n",
    "    dataset = DatasetDict(\n",
    "        {name: Dataset.from_pandas(df, preserve_index=False) for name, df in dataframes.items()}\n",
    "    )\n",
    "    return dataset\n",
    "\n",
    "def save_dataset_as_parquet(dataset_dict, folder_path):\n",
    "    if not os.path.exists(folder_path):\n",
    "        os.makedirs(folder_path)\n",
    "    \n",
    "    for split, dataset in dataset_dict.items():\n",
    "        file_path = os.path.join(folder_path, f\"{split}.parquet\")\n",
    "        df = dataset.to_pandas()\n",
    "        df.to_parquet(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizing the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, DebertaV2Tokenizer\n",
    "import os\n",
    "def tokenize_data(dataset, model_name, model_dir=None, save_and_load=False):\n",
    "    \"\"\"\"Tokenize the given dataset using the given model tokenizer.\"\"\"\n",
    "    if model_name == \"microsoft/deberta-v3-base\":\n",
    "        tokenizer = DebertaV2Tokenizer.from_pretrained(model_name, use_fast=True)\n",
    "    else:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name) \n",
    "        \n",
    "    def tokenize_and_format(batch):\n",
    "        tokens = tokenizer(batch['preprocessed_text'], padding=True, truncation=True)\n",
    "        tokens = {key: torch.tensor(val).to(device) for key, val in tokens.items()}\n",
    "        tokens['labels'] = torch.tensor(batch['label']).to(device)\n",
    "        return tokens\n",
    "    if save_and_load:\n",
    "        if model_dir and os.path.exists(f\"{model_dir}/tokenized_dataset\"):\n",
    "            print(f\"Loading tokenized dataset from {model_dir}\")\n",
    "            tokenized_dataset = load_dataset(f\"{model_dir}/tokenized_dataset\")\n",
    "        else:\n",
    "            print(f\"Tokenizing Data\")\n",
    "            tokenized_dataset = dataset.map(tokenize_and_format, batched=True)\n",
    "            print(f\"Saving tokenized dataset to {model_dir}\")\n",
    "            save_dataset_as_parquet(tokenized_dataset, f\"{model_dir}/tokenized_dataset\")\n",
    "    else:\n",
    "        print(f\"Tokenizing Data\")\n",
    "        tokenized_dataset = dataset.map(tokenize_and_format, batched=True)\n",
    "\n",
    "    return tokenized_dataset, tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting Model Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import os\n",
    "from pathlib import Path\n",
    "def get_model_dir(model_name, use_peft, from_checkpoint):\n",
    "    \n",
    "    root_path = Path().resolve().parent\n",
    "    \n",
    "    if use_peft:\n",
    "        model_dir = f\"{root_path}/models/with_peft/{model_name}\"\n",
    "    else:\n",
    "        model_dir = f\"{root_path}/models/without_peft/{model_name}\"\n",
    "    if not os.path.exists(model_dir):\n",
    "        os.makedirs(model_dir)\n",
    "        \n",
    "        if from_checkpoint:\n",
    "            print(f\"WARNING: Cannot continue training from checkpoint as the model directory is empty. Starting from scratch.\")\n",
    "            from_checkpoint = False\n",
    "        \n",
    "    elif not from_checkpoint:\n",
    "    \n",
    "        input(\"WARNING: The model directory already exists. As 'from_checkpoint' is set to 'False' the content will be overwritten. Press Enter to continue or Ctrl+C to cancel.\")\n",
    "        # Overwrite the existing directory\n",
    "        for filename in os.listdir(model_dir):\n",
    "            file_path = os.path.join(model_dir, filename)\n",
    "            if os.path.isfile(file_path) or os.path.islink(file_path):\n",
    "                os.unlink(file_path)\n",
    "            elif os.path.isdir(file_path):\n",
    "                shutil.rmtree(file_path)\n",
    "    \n",
    "    print(f\"Model directory: {model_dir}\")\n",
    "    return model_dir, from_checkpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Looking for Model Checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "def find_model_file(model_dir):\n",
    "    \"\"\"\n",
    "    Look for a model file in the given directory, with the following priority:\n",
    "    1. model.safetensors or adapter_model.safetensors in the root directory\n",
    "    2. model.safetensors or adapter_model.safetensors in the latest checkpoint directory\n",
    "    Returns None if no model file is found.\n",
    "    \"\"\"\n",
    "    checkpoint_dirs = sorted([\n",
    "        d for d in os.listdir(model_dir) \n",
    "        if os.path.isdir(os.path.join(model_dir, d)) \n",
    "        and d.startswith('checkpoint-')\n",
    "    ], key=lambda x: int(x.split('-')[-1]), reverse=True)\n",
    "    \n",
    "    # Check each checkpoint directory for model files\n",
    "    for checkpoint_dir in checkpoint_dirs:\n",
    "        full_path = os.path.join(model_dir, checkpoint_dir)\n",
    "        if os.path.exists(os.path.join(full_path, \"model.safetensors\")) or \\\n",
    "        os.path.exists(os.path.join(full_path, \"adapter_model.safetensors\")):\n",
    "            print(f\"Found checkpoint model file in: {full_path}\")\n",
    "            return full_path\n",
    "        else:\n",
    "            print(f\"No model file found in {full_path}\")\n",
    "            return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Model Object without PEFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import  AutoModelForSequenceClassification\n",
    "def load_model_object(model_dir, model, config, from_checkpoint=False):\n",
    "    \n",
    "    if not from_checkpoint:\n",
    "        return model\n",
    "\n",
    "    checkpoint_dir = find_model_file(model_dir)\n",
    "         \n",
    "    if checkpoint_dir is None:\n",
    "        print(f\"No checkpoint found in {from_checkpoint}\")\n",
    "        return model \n",
    "    else:\n",
    "        print(f\"Loading full model weights from {checkpoint_dir}\")\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(\n",
    "            checkpoint_dir,\n",
    "            config=config\n",
    "        )\n",
    "        return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup of PEFT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import get_peft_model, LoraConfig, PeftModelForSequenceClassification\n",
    "\n",
    "def setup_peft(model_name, model, model_dir, from_checkpoint=False):\n",
    "    if model_name == \"distilbert-base-uncased\":\n",
    "        target_modules = [\"q_lin\", \"k_lin\", \"v_lin\"]\n",
    "    elif model_name == \"microsoft/deberta-v3-base\":\n",
    "        target_modules = None\n",
    "    else:\n",
    "        target_modules = [\"query\", \"key\", \"value\"]\n",
    "        \n",
    "    # PEFT: LoRA configuration\n",
    "    peft_config = LoraConfig(\n",
    "        task_type=\"SEQ_CLS\",\n",
    "        r=16,\n",
    "        lora_alpha=32,\n",
    "        lora_dropout=0.1,\n",
    "        target_modules=target_modules\n",
    "    )\n",
    "    model = get_peft_model(model, peft_config)\n",
    "    \n",
    "    if from_checkpoint:\n",
    "        checkpoint_dir = find_model_file(model_dir)\n",
    "        if checkpoint_dir:\n",
    "            print(f\"Loading LoRA weights from {checkpoint_dir}\")\n",
    "            from_pretrained_kwargs = {\n",
    "                \"is_trainable\": True,\n",
    "                \"inference_mode\": False\n",
    "            }\n",
    "            model = PeftModelForSequenceClassification.from_pretrained(\n",
    "                model,\n",
    "                checkpoint_dir,\n",
    "                **from_pretrained_kwargs,\n",
    "            )\n",
    "        else:\n",
    "            print(f\"No checkpoint found in {model_dir}\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Previous Training Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "def get_completed_epochs(model_dir):\n",
    "    checkpoint_dir = find_model_file(model_dir)\n",
    "    trainer_state_path = os.path.join(checkpoint_dir, \"trainer_state.json\")\n",
    "    if os.path.exists(trainer_state_path):\n",
    "        with open(trainer_state_path, \"r\") as f:\n",
    "            trainer_state = json.load(f)\n",
    "            completed_epochs = trainer_state.get(\"epoch\", 0)\n",
    "            per_device_train_batch_size = trainer_state.get(\"train_batch_size\", 32)\n",
    "            print(f\"Resuming from checkpoint. Completed epochs: {completed_epochs}, Previous Batch Size: {per_device_train_batch_size}\")\n",
    "        return completed_epochs, per_device_train_batch_size \n",
    "    else:\n",
    "        print(\"Starting from scratch.\")\n",
    "        return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from datetime import datetime\n",
    "import os\n",
    "from pathlib import Path\n",
    "def process_training_logs(data, use_peft, model_name):\n",
    "    \"\"\"\n",
    "    Process training logs into separate training and evaluation dataframes with Parquet storage.\n",
    "    \n",
    "    Args:\n",
    "        data (list): List of dictionaries containing training and evaluation logs\n",
    "        save_dir (str, optional): Directory to save the processed DataFrames as Parquet files\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (training_df, eval_df, summary_df) containing processed DataFrames\n",
    "    \"\"\"\n",
    "    root_path = Path().resolve().parent\n",
    "    if use_peft:\n",
    "        save_dir = f\"{root_path}/data/model_evaluation/with_peft/{model_name}\"\n",
    "    else:\n",
    "        save_dir = f\"{root_path}/data/model_evaluation/without_peft/{model_name}\"\n",
    "    # Initialize log containers\n",
    "    training_logs = []\n",
    "    eval_logs = []\n",
    "    summary_logs = []\n",
    "    \n",
    "    for entry in data:\n",
    "        # Process evaluation logs\n",
    "        if any(key.startswith('eval_') for key in entry.keys()):\n",
    "            eval_entry = {'epoch': entry.get('epoch'), 'step': entry.get('step')}\n",
    "            for key, value in entry.items():\n",
    "                if key.startswith('eval_'):\n",
    "                    clean_key = key[5:]\n",
    "                    eval_entry[clean_key] = value\n",
    "            eval_logs.append(eval_entry)\n",
    "            \n",
    "        # Process training summary logs\n",
    "        elif 'train_loss' in entry:\n",
    "            summary_entry = {\n",
    "                'epoch': entry.get('epoch'),\n",
    "                'step': entry.get('step'),\n",
    "                'total_flos': entry.get('total_flos'),\n",
    "                'train_loss': entry.get('train_loss'),\n",
    "                'train_runtime': entry.get('train_runtime'),\n",
    "                'train_samples_per_second': entry.get('train_samples_per_second'),\n",
    "                'train_steps_per_second': entry.get('train_steps_per_second')\n",
    "            }\n",
    "            summary_logs.append(summary_entry)\n",
    "            \n",
    "        # Process regular training logs\n",
    "        else:\n",
    "            training_logs.append(entry)\n",
    "    \n",
    "    # Create DataFrames\n",
    "    training_df = pd.DataFrame(training_logs)\n",
    "    eval_df = pd.DataFrame(eval_logs)\n",
    "    summary_df = pd.DataFrame(summary_logs)\n",
    "    \n",
    "    # Sort and clean up DataFrames\n",
    "    if not training_df.empty:\n",
    "        training_df = training_df.sort_values(['epoch', 'step']).reset_index(drop=True)\n",
    "        training_df['loss_change'] = training_df['loss'].diff() if 'loss' in training_df.columns else None\n",
    "        training_df['loss_change_rate'] = (training_df['loss_change'] / training_df['loss'].shift(1)) if 'loss' in training_df.columns else None\n",
    "\n",
    "    if not eval_df.empty:\n",
    "        eval_df = eval_df.sort_values(['epoch', 'step']).reset_index(drop=True)\n",
    "        if 'loss' in eval_df.columns:\n",
    "            eval_df['loss_change'] = eval_df['loss'].diff()\n",
    "            eval_df['best_loss_so_far'] = eval_df['loss'].cummin()\n",
    "        if 'accuracy' in eval_df.columns:\n",
    "            eval_df['best_accuracy_so_far'] = eval_df['accuracy'].cummax()\n",
    "\n",
    "    # Save DataFrames if directory is provided\n",
    "    if save_dir:\n",
    "        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "        \n",
    "        # Save DataFrames as Parquet with optimal compression\n",
    "        if not training_df.empty:\n",
    "            training_df.to_parquet(\n",
    "                os.path.join(save_dir, f'training_logs.parquet'),\n",
    "                compression='brotli',  # Typically best compression ratio for ML metrics\n",
    "                index=False\n",
    "            )\n",
    "        if not eval_df.empty:\n",
    "            eval_df.to_parquet(\n",
    "                os.path.join(save_dir, f'eval_logs.parquet'),\n",
    "                compression='brotli',\n",
    "                index=False\n",
    "            )\n",
    "        if not summary_df.empty:\n",
    "            summary_df.to_parquet(\n",
    "                os.path.join(save_dir, f'summary_logs.parquet'),\n",
    "                compression='brotli',\n",
    "                index=False\n",
    "            )\n",
    "            \n",
    "        # Save minimal training configuration summary\n",
    "        config_summary = {\n",
    "            'total_steps': len(training_df) if not training_df.empty else 0,\n",
    "            'total_epochs': float(training_df['epoch'].max()) if not training_df.empty else 0,\n",
    "            'eval_frequency': len(eval_df) / len(training_df) if not training_df.empty and not eval_df.empty else 0,\n",
    "            'final_train_loss': float(training_df['loss'].iloc[-1]) if not training_df.empty and 'loss' in training_df else None,\n",
    "            'best_eval_loss': float(eval_df['loss'].min()) if not eval_df.empty and 'loss' in eval_df else None,\n",
    "            'best_eval_accuracy': float(eval_df['accuracy'].max()) if not eval_df.empty and 'accuracy' in eval_df else None,\n",
    "            'timestamp': timestamp\n",
    "        }\n",
    "        \n",
    "        with open(os.path.join(save_dir, f'training_summary.json'), 'w') as f:\n",
    "            json.dump(config_summary, f)\n",
    "    \n",
    "    return training_df, eval_df, summary_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, roc_curve, auc\n",
    "import torch\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "def process_model_evaluation(trainer, tokenized_dataset, test_results, model_name, use_peft):\n",
    "    \"\"\"\n",
    "    Process and save detailed evaluation metrics for the model.\n",
    "    \n",
    "    Args:\n",
    "        trainer: HuggingFace trainer instance\n",
    "        tokenized_dataset: Dictionary containing the dataset splits\n",
    "        test_results: Dictionary containing initial test results\n",
    "        model_name: Name of the model being evaluated\n",
    "        use_peft: Boolean indicating if PEFT was used\n",
    "        save_dir: Directory to save the evaluation results\n",
    "        \n",
    "    Returns:\n",
    "        dict: Dictionary containing all computed metrics\n",
    "    \"\"\"\n",
    "    \n",
    "    root_path = Path().resolve().parent\n",
    "    if use_peft:\n",
    "        save_dir = f\"{root_path}/data/model_evaluation/with_peft/{model_name}\"\n",
    "    else:\n",
    "        save_dir = f\"{root_path}/data/model_evaluation/without_peft/{model_name}\"\n",
    "        \n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    # Get predictions and labels for the test set\n",
    "    test_pred = trainer.predict(tokenized_dataset[\"test\"])\n",
    "    predictions = np.argmax(test_pred.predictions, axis=1)\n",
    "    labels = test_pred.label_ids\n",
    "    \n",
    "    # Calculate probabilities for ROC curve\n",
    "    probabilities = torch.nn.functional.softmax(torch.tensor(test_pred.predictions), dim=1).numpy()\n",
    "    \n",
    "    # Calculate confusion matrix\n",
    "    cm = confusion_matrix(labels, predictions)\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    \n",
    "    # Calculate ROC curve and AUC\n",
    "    fpr, tpr, _ = roc_curve(labels, probabilities[:, 1])\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    \n",
    "    # Compile all metrics\n",
    "    metrics_dict = {\n",
    "        'model_name': model_name,\n",
    "        'use_peft': use_peft,\n",
    "        'accuracy': test_results['eval_accuracy'],\n",
    "        'precision': test_results['eval_precision'],\n",
    "        'recall': test_results['eval_recall'],\n",
    "        'f1': test_results['eval_f1'],\n",
    "        'loss': test_results['eval_loss'],\n",
    "        'roc_auc': roc_auc,\n",
    "        'true_negatives': int(tn),\n",
    "        'false_positives': int(fp),\n",
    "        'false_negatives': int(fn),\n",
    "        'true_positives': int(tp)\n",
    "    }\n",
    "    \n",
    "    # Create DataFrames for different aspects of evaluation\n",
    "    main_metrics_df = pd.DataFrame([metrics_dict])\n",
    "    \n",
    "    # Create confusion matrix DataFrame\n",
    "    confusion_df = pd.DataFrame({\n",
    "        'model_name': [model_name],\n",
    "        'predicted_negative_actual_negative': [tn],\n",
    "        'predicted_positive_actual_negative': [fp],\n",
    "        'predicted_negative_actual_positive': [fn],\n",
    "        'predicted_positive_actual_positive': [tp]\n",
    "    })\n",
    "    \n",
    "    # Create ROC curve DataFrame\n",
    "    roc_df = pd.DataFrame({\n",
    "        'model_name': model_name,\n",
    "        'false_positive_rate': fpr,\n",
    "        'true_positive_rate': tpr,\n",
    "        'auc': roc_auc\n",
    "    })\n",
    "    \n",
    "    # Create predictions DataFrame\n",
    "    predictions_df = pd.DataFrame({\n",
    "        'model_name': model_name,\n",
    "        'true_label': labels,\n",
    "        'predicted_label': predictions,\n",
    "        'confidence_negative': probabilities[:, 0],\n",
    "        'confidence_positive': probabilities[:, 1]\n",
    "    })\n",
    "    \n",
    "    \n",
    "    main_metrics_df.to_parquet(f'{save_dir}/sklearn_metrics.parquet', compression='brotli', index=False)\n",
    "    confusion_df.to_parquet(f'{save_dir}/sklearn_confusion.parquet', compression='brotli', index=False)\n",
    "    roc_df.to_parquet(f'{save_dir}/sklearn_roc.parquet', compression='brotli', index=False)\n",
    "    predictions_df.to_parquet(f'{save_dir}/sklearn_predictions.parquet', compression='brotli', index=False)\n",
    "    \n",
    "    return metrics_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code for Transformer Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoConfig, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "from pprint import pprint\n",
    "def fine_tune_model(\n",
    "    model_name, \n",
    "    dataset, \n",
    "    training_batch_size=32, \n",
    "    epochs=5,\n",
    "    use_peft=True,\n",
    "    from_checkpoint=True\n",
    "):\n",
    "    print(f\"Using Model: {model_name} with device {device}\")\n",
    "    print(f\"Training mode: {'PEFT' if use_peft else 'Full model'}\")\n",
    "    \n",
    "    model_dir, from_checkpoint = get_model_dir(model_name, use_peft, from_checkpoint)\n",
    "    \n",
    "    tokenized_dataset, tokenizer = tokenize_data(dataset, model_name, model_dir)\n",
    "\n",
    "    config = AutoConfig.from_pretrained(model_name, num_labels=2)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_name, config=config).to(device)\n",
    "    \n",
    "    if use_peft:\n",
    "        model = setup_peft(model_name, model, model_dir, from_checkpoint=from_checkpoint)\n",
    "        \n",
    "    else:\n",
    "        model = load_model_object(model_dir, model, config, from_checkpoint=from_checkpoint)\n",
    "\n",
    "    model = model.to(device)\n",
    "    \n",
    "    if from_checkpoint:\n",
    "        completed_epochs, old_batch_size = get_completed_epochs(model_dir)\n",
    "        if completed_epochs is not None:\n",
    "            epochs = completed_epochs + epochs\n",
    "        if old_batch_size is not None:\n",
    "            training_batch_size = old_batch_size\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=model_dir,\n",
    "        num_train_epochs=epochs,\n",
    "        per_device_train_batch_size=training_batch_size,\n",
    "        per_device_eval_batch_size=training_batch_size,\n",
    "        learning_rate=2e-5,\n",
    "        weight_decay=0.01,\n",
    "        evaluation_strategy=\"steps\",\n",
    "        eval_steps=200,\n",
    "        save_strategy=\"steps\",\n",
    "        save_steps=200,\n",
    "        logging_dir=f\"{model_dir}/logs\",\n",
    "        save_total_limit=4,\n",
    "        fp16=True,\n",
    "        logging_steps=50,\n",
    "        report_to=\"tensorboard\",\n",
    "        lr_scheduler_type=\"linear\",\n",
    "        warmup_steps=500,\n",
    "        metric_for_best_model=\"eval_loss\",\n",
    "        load_best_model_at_end=True,\n",
    "    )\n",
    "    \n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_dataset[\"train\"],\n",
    "        eval_dataset=tokenized_dataset[\"validation\"],\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "    \n",
    "    print(\"Starting Training\")\n",
    "    \n",
    "    trainer.train(resume_from_checkpoint=from_checkpoint)\n",
    "    test_results = trainer.evaluate(eval_dataset=tokenized_dataset[\"test\"])\n",
    "    print(\"Test Results:\")\n",
    "    pprint(f\"{test_results}\")\n",
    "    \n",
    "    process_model_evaluation(\n",
    "        trainer=trainer,\n",
    "        tokenized_dataset=tokenized_dataset,\n",
    "        test_results=test_results,\n",
    "        model_name=model_name,\n",
    "        use_peft=use_peft,\n",
    "    )\n",
    "    process_training_logs(trainer.state.log_history, use_peft, model_name)\n",
    "    \n",
    "    if use_peft:\n",
    "        model.save_pretrained(model_dir)\n",
    "    else:\n",
    "        trainer.save_model(model_dir)\n",
    "    tokenizer.save_pretrained(model_dir)\n",
    "    \n",
    "    torch.cuda.empty_cache()\n",
    "    print(f\"Finished training {model_name}. Model saved to {model_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Model: distilbert-base-uncased with device cuda\n",
      "Training mode: PEFT\n",
      "Model directory: /home/nikl/programming/fhdw/knowledge_engineering_ausarbeitung/fakenews_detection/models/with_peft/distilbert-base-uncased\n",
      "Tokenizing Data\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9bcec95b3e4e40f4a82d0b274bb537c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/44153 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f0d3d3e7b5a413bb9fa60eaa59fa398",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/12616 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab434c8093ee47528a1fd1c1efe3ff62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6308 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Training\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1380' max='1380' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1380/1380 13:14, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.675000</td>\n",
       "      <td>0.670359</td>\n",
       "      <td>0.559845</td>\n",
       "      <td>0.036439</td>\n",
       "      <td>0.990566</td>\n",
       "      <td>0.018561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.528300</td>\n",
       "      <td>0.479799</td>\n",
       "      <td>0.786937</td>\n",
       "      <td>0.758621</td>\n",
       "      <td>0.770944</td>\n",
       "      <td>0.746686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.369100</td>\n",
       "      <td>0.346627</td>\n",
       "      <td>0.848288</td>\n",
       "      <td>0.829320</td>\n",
       "      <td>0.836782</td>\n",
       "      <td>0.821990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.297900</td>\n",
       "      <td>0.294552</td>\n",
       "      <td>0.874366</td>\n",
       "      <td>0.859871</td>\n",
       "      <td>0.860099</td>\n",
       "      <td>0.859643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.280700</td>\n",
       "      <td>0.270455</td>\n",
       "      <td>0.885938</td>\n",
       "      <td>0.869431</td>\n",
       "      <td>0.893177</td>\n",
       "      <td>0.846915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.244700</td>\n",
       "      <td>0.252062</td>\n",
       "      <td>0.897511</td>\n",
       "      <td>0.885220</td>\n",
       "      <td>0.889087</td>\n",
       "      <td>0.881386</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Results:\n",
      "(\"{'eval_loss': 0.24135418236255646, 'eval_accuracy': 0.901712111604312, \"\n",
      " \"'eval_f1': 0.8904980572235959, 'eval_precision': 0.8898693963995764, \"\n",
      " \"'eval_recall': 0.8911276069282432, 'eval_runtime': 26.2179, \"\n",
      " \"'eval_samples_per_second': 240.599, 'eval_steps_per_second': 7.552, 'epoch': \"\n",
      " '1.0}')\n",
      "Finished training distilbert-base-uncased. Model saved to /home/nikl/programming/fhdw/knowledge_engineering_ausarbeitung/fakenews_detection/models/with_peft/distilbert-base-uncased\n"
     ]
    }
   ],
   "source": [
    "from warnings import filterwarnings\n",
    "filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "possible_models = {\"bert\": \"bert-base-uncased\", \"distilbert\": \"distilbert-base-uncased\", \"roberta\": \"roberta-base\", \"deberta\": \"microsoft/deberta-v3-base\"}\n",
    "\n",
    "current_model = possible_models[\"distilbert\"]\n",
    "\n",
    "fine_tune_model(current_model, dataset, training_batch_size=32, epochs=1, use_peft=True, from_checkpoint=False)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
