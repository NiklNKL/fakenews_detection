{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Infrastructure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data into a dataset dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['preprocessed_text', 'label', 'label_names'],\n",
       "        num_rows: 44153\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['preprocessed_text', 'label', 'label_names'],\n",
       "        num_rows: 12616\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['preprocessed_text', 'label', 'label_names'],\n",
       "        num_rows: 6308\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "train = pd.read_parquet(\"/home/developing_nacho/fhdw/knowledge_engineering/fakenews_detection/data/model_training/train_df.parquet\")\n",
    "valid = pd.read_parquet(\"/home/developing_nacho/fhdw/knowledge_engineering/fakenews_detection/data/model_training/valid_df.parquet\")\n",
    "test = pd.read_parquet(\"/home/developing_nacho/fhdw/knowledge_engineering/fakenews_detection/data/model_training/test_df.parquet\")\n",
    "\n",
    "dataset = DatasetDict(\n",
    "    {'train':Dataset.from_pandas(train,preserve_index=False),\n",
    "     'validation': Dataset.from_pandas(valid,preserve_index=False),\n",
    "     'test':Dataset.from_pandas(test,preserve_index=False)\n",
    "     }    \n",
    ")\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method for evaluating performance while training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-02 23:51:48.144052: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-02-02 23:51:48.430439: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2025-02-02 23:51:48.430453: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import evaluate\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "accuracy_metric = evaluate.load(\"accuracy\")\n",
    "f1_metric = evaluate.load(\"f1\")\n",
    "precision_metric = evaluate.load(\"precision\")\n",
    "recall_metric = evaluate.load(\"recall\")\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    logits, labels = pred\n",
    "    predictions = torch.argmax(torch.tensor(logits), dim=-1)\n",
    "    result = {}\n",
    "    for metric in [accuracy_metric, f1_metric, precision_metric, recall_metric]:\n",
    "        result.update(metric.compute(predictions=predictions, references=labels))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def load_dataset(path):\n",
    "    parquet_files = glob.glob(f\"{path}/*.parquet\")\n",
    "    dataframes = {file.split('/')[-1].replace('.parquet', ''): pd.read_parquet(file) for file in parquet_files}\n",
    "    \n",
    "    dataset = DatasetDict(\n",
    "        {name: Dataset.from_pandas(df, preserve_index=False) for name, df in dataframes.items()}\n",
    "    )\n",
    "    return dataset\n",
    "\n",
    "def save_dataset_as_parquet(dataset_dict, folder_path):\n",
    "    if not os.path.exists(folder_path):\n",
    "        os.makedirs(folder_path)\n",
    "    \n",
    "    for split, dataset in dataset_dict.items():\n",
    "        file_path = os.path.join(folder_path, f\"{split}.parquet\")\n",
    "        df = dataset.to_pandas()\n",
    "        df.to_parquet(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizing the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, DebertaV2Tokenizer\n",
    "import os\n",
    "def tokenize_data(dataset, model_name, model_dir=None, save_and_load=False):\n",
    "    \"\"\"\"Tokenize the given dataset using the given model tokenizer.\"\"\"\n",
    "    if model_name == \"microsoft/deberta-v3-base\":\n",
    "        tokenizer = DebertaV2Tokenizer.from_pretrained(model_name, use_fast=True)\n",
    "    else:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name) \n",
    "        \n",
    "    def tokenize_and_format(batch):\n",
    "        tokens = tokenizer(batch['preprocessed_text'], padding=True, truncation=True)\n",
    "        tokens = {key: torch.tensor(val).to(device) for key, val in tokens.items()}\n",
    "        tokens['labels'] = torch.tensor(batch['label']).to(device)\n",
    "        return tokens\n",
    "    if save_and_load:\n",
    "        if model_dir and os.path.exists(f\"{model_dir}/tokenized_dataset\"):\n",
    "            print(f\"Loading tokenized dataset from {model_dir}\")\n",
    "            tokenized_dataset = load_dataset(f\"{model_dir}/tokenized_dataset\")\n",
    "        else:\n",
    "            print(f\"Tokenizing Data\")\n",
    "            tokenized_dataset = dataset.map(tokenize_and_format, batched=True)\n",
    "            print(f\"Saving tokenized dataset to {model_dir}\")\n",
    "            save_dataset_as_parquet(tokenized_dataset, f\"{model_dir}/tokenized_dataset\")\n",
    "    else:\n",
    "        print(f\"Tokenizing Data\")\n",
    "        tokenized_dataset = dataset.map(tokenize_and_format, batched=True)\n",
    "\n",
    "    return tokenized_dataset, tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting Model Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import os\n",
    "from pathlib import Path\n",
    "def get_model_dir(model_name, use_peft, from_checkpoint):\n",
    "    \n",
    "    root_path = Path().resolve().parent\n",
    "    \n",
    "    if use_peft:\n",
    "        model_dir = f\"{root_path}/models/with_peft/{model_name}\"\n",
    "    else:\n",
    "        model_dir = f\"{root_path}/models/without_peft/{model_name}\"\n",
    "    if not os.path.exists(model_dir):\n",
    "        os.makedirs(model_dir)\n",
    "        \n",
    "        if from_checkpoint:\n",
    "            print(f\"WARNING: Cannot continue training from checkpoint as the model directory is empty. Starting from scratch.\")\n",
    "            from_checkpoint = False\n",
    "        \n",
    "    elif not from_checkpoint:\n",
    "    \n",
    "        input(\"WARNING: The model directory already exists. As 'from_checkpoint' is set to 'False' the content will be overwritten. Press Enter to continue or Ctrl+C to cancel.\")\n",
    "        # Overwrite the existing directory\n",
    "        for filename in os.listdir(model_dir):\n",
    "            file_path = os.path.join(model_dir, filename)\n",
    "            if os.path.isfile(file_path) or os.path.islink(file_path):\n",
    "                os.unlink(file_path)\n",
    "            elif os.path.isdir(file_path):\n",
    "                shutil.rmtree(file_path)\n",
    "    \n",
    "    print(f\"Model directory: {model_dir}\")\n",
    "    return model_dir, from_checkpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Looking for Model Checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "def find_model_file(model_dir):\n",
    "    \"\"\"\n",
    "    Look for a model file in the given directory, with the following priority:\n",
    "    1. model.safetensors or adapter_model.safetensors in the root directory\n",
    "    2. model.safetensors or adapter_model.safetensors in the latest checkpoint directory\n",
    "    Returns None if no model file is found.\n",
    "    \"\"\"\n",
    "    checkpoint_dirs = sorted([\n",
    "        d for d in os.listdir(model_dir) \n",
    "        if os.path.isdir(os.path.join(model_dir, d)) \n",
    "        and d.startswith('checkpoint-')\n",
    "    ], key=lambda x: int(x.split('-')[-1]), reverse=True)\n",
    "    \n",
    "    # Check each checkpoint directory for model files\n",
    "    for checkpoint_dir in checkpoint_dirs:\n",
    "        full_path = os.path.join(model_dir, checkpoint_dir)\n",
    "        if os.path.exists(os.path.join(full_path, \"model.safetensors\")) or \\\n",
    "        os.path.exists(os.path.join(full_path, \"adapter_model.safetensors\")):\n",
    "            print(f\"Found checkpoint model file in: {full_path}\")\n",
    "            return full_path\n",
    "        else:\n",
    "            print(f\"No model file found in {full_path}\")\n",
    "            return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Model Object without PEFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import  AutoModelForSequenceClassification\n",
    "def load_model_object(model_dir, model, config, from_checkpoint=False):\n",
    "    \n",
    "    if not from_checkpoint:\n",
    "        return model\n",
    "\n",
    "    checkpoint_dir = find_model_file(model_dir)\n",
    "         \n",
    "    if checkpoint_dir is None:\n",
    "        print(f\"No checkpoint found in {from_checkpoint}\")\n",
    "        return model \n",
    "    else:\n",
    "        print(f\"Loading full model weights from {checkpoint_dir}\")\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(\n",
    "            checkpoint_dir,\n",
    "            config=config\n",
    "        )\n",
    "        return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup of PEFT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import get_peft_model, LoraConfig, PeftModelForSequenceClassification\n",
    "\n",
    "def setup_peft(model_name, model, model_dir, from_checkpoint=False):\n",
    "    if model_name == \"distilbert-base-uncased\":\n",
    "        target_modules = [\"q_lin\", \"k_lin\", \"v_lin\"]\n",
    "    elif model_name == \"microsoft/deberta-v3-base\":\n",
    "        target_modules = None\n",
    "    else:\n",
    "        target_modules = [\"query\", \"key\", \"value\"]\n",
    "        \n",
    "    # PEFT: LoRA configuration\n",
    "    peft_config = LoraConfig(\n",
    "        task_type=\"SEQ_CLS\",\n",
    "        r=16,\n",
    "        lora_alpha=32,\n",
    "        lora_dropout=0.1,\n",
    "        target_modules=target_modules\n",
    "    )\n",
    "    model = get_peft_model(model, peft_config)\n",
    "    \n",
    "    if from_checkpoint:\n",
    "        checkpoint_dir = find_model_file(model_dir)\n",
    "        if checkpoint_dir:\n",
    "            print(f\"Loading LoRA weights from {checkpoint_dir}\")\n",
    "            from_pretrained_kwargs = {\n",
    "                \"is_trainable\": True,\n",
    "                \"inference_mode\": False\n",
    "            }\n",
    "            model = PeftModelForSequenceClassification.from_pretrained(\n",
    "                model,\n",
    "                checkpoint_dir,\n",
    "                **from_pretrained_kwargs,\n",
    "            )\n",
    "        else:\n",
    "            print(f\"No checkpoint found in {model_dir}\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Previous Training Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "def get_completed_epochs(model_dir):\n",
    "    checkpoint_dir = find_model_file(model_dir)\n",
    "    trainer_state_path = os.path.join(checkpoint_dir, \"trainer_state.json\")\n",
    "    if os.path.exists(trainer_state_path):\n",
    "        with open(trainer_state_path, \"r\") as f:\n",
    "            trainer_state = json.load(f)\n",
    "            completed_epochs = trainer_state.get(\"epoch\", 0)\n",
    "            per_device_train_batch_size = trainer_state.get(\"train_batch_size\", 32)\n",
    "            print(f\"Resuming from checkpoint. Completed epochs: {completed_epochs}, Previous Batch Size: {per_device_train_batch_size}\")\n",
    "        return completed_epochs, per_device_train_batch_size \n",
    "    else:\n",
    "        print(\"Starting from scratch.\")\n",
    "        return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from datetime import datetime\n",
    "import os\n",
    "from pathlib import Path\n",
    "def process_training_logs(data, use_peft, model_name):\n",
    "    \"\"\"\n",
    "    Process training logs into separate training and evaluation dataframes with Parquet storage.\n",
    "    \n",
    "    Args:\n",
    "        data (list): List of dictionaries containing training and evaluation logs\n",
    "        save_dir (str, optional): Directory to save the processed DataFrames as Parquet files\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (training_df, eval_df, summary_df) containing processed DataFrames\n",
    "    \"\"\"\n",
    "    root_path = Path().resolve().parent\n",
    "    if use_peft:\n",
    "        save_dir = f\"{root_path}/data/model_evaluation/with_peft/{model_name}\"\n",
    "    else:\n",
    "        save_dir = f\"{root_path}/data/model_evaluation/without_peft/{model_name}\"\n",
    "    # Initialize log containers\n",
    "    training_logs = []\n",
    "    eval_logs = []\n",
    "    summary_logs = []\n",
    "    \n",
    "    for entry in data:\n",
    "        # Process evaluation logs\n",
    "        if any(key.startswith('eval_') for key in entry.keys()):\n",
    "            eval_entry = {'epoch': entry.get('epoch'), 'step': entry.get('step')}\n",
    "            for key, value in entry.items():\n",
    "                if key.startswith('eval_'):\n",
    "                    clean_key = key[5:]\n",
    "                    eval_entry[clean_key] = value\n",
    "            eval_logs.append(eval_entry)\n",
    "            \n",
    "        # Process training summary logs\n",
    "        elif 'train_loss' in entry:\n",
    "            summary_entry = {\n",
    "                'epoch': entry.get('epoch'),\n",
    "                'step': entry.get('step'),\n",
    "                'total_flos': entry.get('total_flos'),\n",
    "                'train_loss': entry.get('train_loss'),\n",
    "                'train_runtime': entry.get('train_runtime'),\n",
    "                'train_samples_per_second': entry.get('train_samples_per_second'),\n",
    "                'train_steps_per_second': entry.get('train_steps_per_second')\n",
    "            }\n",
    "            summary_logs.append(summary_entry)\n",
    "            \n",
    "        # Process regular training logs\n",
    "        else:\n",
    "            training_logs.append(entry)\n",
    "    \n",
    "    # Create DataFrames\n",
    "    training_df = pd.DataFrame(training_logs)\n",
    "    eval_df = pd.DataFrame(eval_logs)\n",
    "    summary_df = pd.DataFrame(summary_logs)\n",
    "    \n",
    "    # Sort and clean up DataFrames\n",
    "    if not training_df.empty:\n",
    "        training_df = training_df.sort_values(['epoch', 'step']).reset_index(drop=True)\n",
    "        training_df['loss_change'] = training_df['loss'].diff() if 'loss' in training_df.columns else None\n",
    "        training_df['loss_change_rate'] = (training_df['loss_change'] / training_df['loss'].shift(1)) if 'loss' in training_df.columns else None\n",
    "\n",
    "    if not eval_df.empty:\n",
    "        eval_df = eval_df.sort_values(['epoch', 'step']).reset_index(drop=True)\n",
    "        if 'loss' in eval_df.columns:\n",
    "            eval_df['loss_change'] = eval_df['loss'].diff()\n",
    "            eval_df['best_loss_so_far'] = eval_df['loss'].cummin()\n",
    "        if 'accuracy' in eval_df.columns:\n",
    "            eval_df['best_accuracy_so_far'] = eval_df['accuracy'].cummax()\n",
    "\n",
    "    # Save DataFrames if directory is provided\n",
    "    if save_dir:\n",
    "        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "        \n",
    "        # Save DataFrames as Parquet with optimal compression\n",
    "        if not training_df.empty:\n",
    "            training_df.to_parquet(\n",
    "                os.path.join(save_dir, f'training_logs.parquet'),\n",
    "                compression='brotli',  # Typically best compression ratio for ML metrics\n",
    "                index=False\n",
    "            )\n",
    "        if not eval_df.empty:\n",
    "            eval_df.to_parquet(\n",
    "                os.path.join(save_dir, f'eval_logs.parquet'),\n",
    "                compression='brotli',\n",
    "                index=False\n",
    "            )\n",
    "        if not summary_df.empty:\n",
    "            summary_df.to_parquet(\n",
    "                os.path.join(save_dir, f'summary_logs.parquet'),\n",
    "                compression='brotli',\n",
    "                index=False\n",
    "            )\n",
    "            \n",
    "        # Save minimal training configuration summary\n",
    "        config_summary = {\n",
    "            'total_steps': len(training_df) if not training_df.empty else 0,\n",
    "            'total_epochs': float(training_df['epoch'].max()) if not training_df.empty else 0,\n",
    "            'eval_frequency': len(eval_df) / len(training_df) if not training_df.empty and not eval_df.empty else 0,\n",
    "            'final_train_loss': float(training_df['loss'].iloc[-1]) if not training_df.empty and 'loss' in training_df else None,\n",
    "            'best_eval_loss': float(eval_df['loss'].min()) if not eval_df.empty and 'loss' in eval_df else None,\n",
    "            'best_eval_accuracy': float(eval_df['accuracy'].max()) if not eval_df.empty and 'accuracy' in eval_df else None,\n",
    "            'timestamp': timestamp\n",
    "        }\n",
    "        \n",
    "        with open(os.path.join(save_dir, f'training_summary.json'), 'w') as f:\n",
    "            json.dump(config_summary, f)\n",
    "    \n",
    "    return training_df, eval_df, summary_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, roc_curve, auc\n",
    "import torch\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "def process_model_evaluation(trainer, tokenized_dataset, test_results, model_name, use_peft):\n",
    "    \"\"\"\n",
    "    Process and save detailed evaluation metrics for the model.\n",
    "    \n",
    "    Args:\n",
    "        trainer: HuggingFace trainer instance\n",
    "        tokenized_dataset: Dictionary containing the dataset splits\n",
    "        test_results: Dictionary containing initial test results\n",
    "        model_name: Name of the model being evaluated\n",
    "        use_peft: Boolean indicating if PEFT was used\n",
    "        save_dir: Directory to save the evaluation results\n",
    "        \n",
    "    Returns:\n",
    "        dict: Dictionary containing all computed metrics\n",
    "    \"\"\"\n",
    "    \n",
    "    root_path = Path().resolve().parent\n",
    "    if use_peft:\n",
    "        save_dir = f\"{root_path}/data/model_evaluation/with_peft/{model_name}\"\n",
    "    else:\n",
    "        save_dir = f\"{root_path}/data/model_evaluation/without_peft/{model_name}\"\n",
    "        \n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    # Get predictions and labels for the test set\n",
    "    test_pred = trainer.predict(tokenized_dataset[\"test\"])\n",
    "    predictions = np.argmax(test_pred.predictions, axis=1)\n",
    "    labels = test_pred.label_ids\n",
    "    \n",
    "    # Calculate probabilities for ROC curve\n",
    "    probabilities = torch.nn.functional.softmax(torch.tensor(test_pred.predictions), dim=1).numpy()\n",
    "    \n",
    "    # Calculate confusion matrix\n",
    "    cm = confusion_matrix(labels, predictions)\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    \n",
    "    # Calculate ROC curve and AUC\n",
    "    fpr, tpr, _ = roc_curve(labels, probabilities[:, 1])\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    \n",
    "    # Compile all metrics\n",
    "    metrics_dict = {\n",
    "        'model_name': model_name,\n",
    "        'use_peft': use_peft,\n",
    "        'accuracy': test_results['eval_accuracy'],\n",
    "        'precision': test_results['eval_precision'],\n",
    "        'recall': test_results['eval_recall'],\n",
    "        'f1': test_results['eval_f1'],\n",
    "        'loss': test_results['eval_loss'],\n",
    "        'roc_auc': roc_auc,\n",
    "        'true_negatives': int(tn),\n",
    "        'false_positives': int(fp),\n",
    "        'false_negatives': int(fn),\n",
    "        'true_positives': int(tp)\n",
    "    }\n",
    "    \n",
    "    # Create DataFrames for different aspects of evaluation\n",
    "    main_metrics_df = pd.DataFrame([metrics_dict])\n",
    "    \n",
    "    # Create confusion matrix DataFrame\n",
    "    confusion_df = pd.DataFrame({\n",
    "        'model_name': [model_name],\n",
    "        'predicted_negative_actual_negative': [tn],\n",
    "        'predicted_positive_actual_negative': [fp],\n",
    "        'predicted_negative_actual_positive': [fn],\n",
    "        'predicted_positive_actual_positive': [tp]\n",
    "    })\n",
    "    \n",
    "    # Create ROC curve DataFrame\n",
    "    roc_df = pd.DataFrame({\n",
    "        'model_name': model_name,\n",
    "        'false_positive_rate': fpr,\n",
    "        'true_positive_rate': tpr,\n",
    "        'auc': roc_auc\n",
    "    })\n",
    "    \n",
    "    # Create predictions DataFrame\n",
    "    predictions_df = pd.DataFrame({\n",
    "        'model_name': model_name,\n",
    "        'true_label': labels,\n",
    "        'predicted_label': predictions,\n",
    "        'confidence_negative': probabilities[:, 0],\n",
    "        'confidence_positive': probabilities[:, 1]\n",
    "    })\n",
    "    \n",
    "    \n",
    "    main_metrics_df.to_parquet(f'{save_dir}/sklearn_metrics.parquet', compression='brotli', index=False)\n",
    "    confusion_df.to_parquet(f'{save_dir}/sklearn_confusion.parquet', compression='brotli', index=False)\n",
    "    roc_df.to_parquet(f'{save_dir}/sklearn_roc.parquet', compression='brotli', index=False)\n",
    "    predictions_df.to_parquet(f'{save_dir}/sklearn_predictions.parquet', compression='brotli', index=False)\n",
    "    \n",
    "    return metrics_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code for Transformer Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoConfig, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "from pprint import pprint\n",
    "def fine_tune_model(\n",
    "    model_name, \n",
    "    dataset, \n",
    "    training_batch_size=32, \n",
    "    epochs=5,\n",
    "    use_peft=True,\n",
    "    from_checkpoint=True\n",
    "):\n",
    "    print(f\"Using Model: {model_name} with device {device}\")\n",
    "    print(f\"Training mode: {'PEFT' if use_peft else 'Full model'}\")\n",
    "    \n",
    "    model_dir, from_checkpoint = get_model_dir(model_name, use_peft, from_checkpoint)\n",
    "    \n",
    "    tokenized_dataset, tokenizer = tokenize_data(dataset, model_name, model_dir)\n",
    "\n",
    "    config = AutoConfig.from_pretrained(model_name, num_labels=2)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_name, config=config).to(device)\n",
    "    \n",
    "    if use_peft:\n",
    "        model = setup_peft(model_name, model, model_dir, from_checkpoint=from_checkpoint)\n",
    "        \n",
    "    else:\n",
    "        model = load_model_object(model_dir, model, config, from_checkpoint=from_checkpoint)\n",
    "\n",
    "    model = model.to(device)\n",
    "    \n",
    "    if from_checkpoint:\n",
    "        completed_epochs, old_batch_size = get_completed_epochs(model_dir)\n",
    "        if completed_epochs is not None:\n",
    "            epochs = completed_epochs + epochs\n",
    "        if old_batch_size is not None:\n",
    "            training_batch_size = old_batch_size\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=model_dir,\n",
    "        num_train_epochs=epochs,\n",
    "        per_device_train_batch_size=training_batch_size,\n",
    "        per_device_eval_batch_size=training_batch_size,\n",
    "        learning_rate=2e-5,\n",
    "        weight_decay=0.01,\n",
    "        evaluation_strategy=\"steps\",\n",
    "        eval_steps=200,\n",
    "        save_strategy=\"steps\",\n",
    "        save_steps=200,\n",
    "        logging_dir=f\"{model_dir}/logs\",\n",
    "        save_total_limit=4,\n",
    "        fp16=True,\n",
    "        logging_steps=50,\n",
    "        report_to=\"tensorboard\",\n",
    "        lr_scheduler_type=\"linear\",\n",
    "        warmup_steps=500,\n",
    "        metric_for_best_model=\"eval_loss\",\n",
    "        load_best_model_at_end=True,\n",
    "    )\n",
    "    \n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_dataset[\"train\"],\n",
    "        eval_dataset=tokenized_dataset[\"validation\"],\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "    \n",
    "    print(\"Starting Training\")\n",
    "    \n",
    "    trainer.train(resume_from_checkpoint=from_checkpoint)\n",
    "    test_results = trainer.evaluate(eval_dataset=tokenized_dataset[\"test\"])\n",
    "    print(\"Test Results:\")\n",
    "    pprint(f\"{test_results}\")\n",
    "    \n",
    "    process_model_evaluation(\n",
    "        trainer=trainer,\n",
    "        tokenized_dataset=tokenized_dataset,\n",
    "        test_results=test_results,\n",
    "        model_name=model_name,\n",
    "        use_peft=use_peft,\n",
    "    )\n",
    "    process_training_logs(trainer.state.log_history, use_peft, model_name)\n",
    "    \n",
    "    if use_peft:\n",
    "        model.save_pretrained(model_dir)\n",
    "    else:\n",
    "        trainer.save_model(model_dir)\n",
    "    tokenizer.save_pretrained(model_dir)\n",
    "    \n",
    "    torch.cuda.empty_cache()\n",
    "    print(f\"Finished training {model_name}. Model saved to {model_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Model: distilbert-base-uncased with device cuda\n",
      "Training mode: PEFT\n",
      "Model directory: /home/developing_nacho/fhdw/knowledge_engineering/fakenews_detection/models/with_peft/distilbert-base-uncased\n",
      "Tokenizing Data\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e7cdc3c32b24e74a1709ad278ad60ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/44153 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "653adc33729d493d9d77544572b19f83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/12616 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37af88c464f44f9a9ef32fd61d541fe8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6308 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Training\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1380' max='1380' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1380/1380 06:52, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.680300</td>\n",
       "      <td>0.672272</td>\n",
       "      <td>0.567850</td>\n",
       "      <td>0.072158</td>\n",
       "      <td>0.968037</td>\n",
       "      <td>0.037476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.540800</td>\n",
       "      <td>0.481963</td>\n",
       "      <td>0.793358</td>\n",
       "      <td>0.763495</td>\n",
       "      <td>0.784197</td>\n",
       "      <td>0.743857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.349100</td>\n",
       "      <td>0.322739</td>\n",
       "      <td>0.861842</td>\n",
       "      <td>0.842305</td>\n",
       "      <td>0.862676</td>\n",
       "      <td>0.822874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.290600</td>\n",
       "      <td>0.265652</td>\n",
       "      <td>0.886969</td>\n",
       "      <td>0.876301</td>\n",
       "      <td>0.860330</td>\n",
       "      <td>0.892876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.262000</td>\n",
       "      <td>0.245312</td>\n",
       "      <td>0.894023</td>\n",
       "      <td>0.881124</td>\n",
       "      <td>0.886404</td>\n",
       "      <td>0.875906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.226900</td>\n",
       "      <td>0.234588</td>\n",
       "      <td>0.898779</td>\n",
       "      <td>0.888773</td>\n",
       "      <td>0.876030</td>\n",
       "      <td>0.901891</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Results:\n",
      "(\"{'eval_loss': 0.22801239788532257, 'eval_accuracy': 0.904565630944832, \"\n",
      " \"'eval_f1': 0.8948655256723717, 'eval_precision': 0.8843631342768381, \"\n",
      " \"'eval_recall': 0.9056203605514316, 'eval_runtime': 13.5983, \"\n",
      " \"'eval_samples_per_second': 463.883, 'eval_steps_per_second': 14.561, \"\n",
      " \"'epoch': 1.0}\")\n",
      "Finished training distilbert-base-uncased. Model saved to /home/developing_nacho/fhdw/knowledge_engineering/fakenews_detection/models/with_peft/distilbert-base-uncased\n"
     ]
    }
   ],
   "source": [
    "from warnings import filterwarnings\n",
    "filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "possible_models = {\"bert\": \"bert-base-uncased\", \"distilbert\": \"distilbert-base-uncased\", \"roberta\": \"roberta-base\"}\n",
    "\n",
    "# , \"deberta\": \"microsoft/deberta-v3-base\"\n",
    "# current_model = possible_models[\"distilbert\"]\n",
    "\n",
    "for current_model in possible_models.values():\n",
    "    fine_tune_model(current_model, dataset, training_batch_size=32, epochs=8, use_peft=True, from_checkpoint=False)\n",
    "\n",
    "for current_model in possible_models.values():\n",
    "    fine_tune_model(current_model, dataset, training_batch_size=32, epochs=8, use_peft=False, from_checkpoint=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Sklearn Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Logistic_Regression...\n",
      "Logistic_Regression accuracy: 0.9461\n",
      "Precision: 0.9430, Recall: 0.9364, F1 Score: 0.9397\n",
      "Training Passive_Aggressive...\n",
      "Passive_Aggressive accuracy: 0.9396\n",
      "Precision: 0.9316, Recall: 0.9339, F1 Score: 0.9327\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Dataset</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>tn</th>\n",
       "      <th>fp</th>\n",
       "      <th>fn</th>\n",
       "      <th>tp</th>\n",
       "      <th>Training_Time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Logistic_Regression</td>\n",
       "      <td>Train</td>\n",
       "      <td>0.956152</td>\n",
       "      <td>0.950883</td>\n",
       "      <td>0.951364</td>\n",
       "      <td>0.951123</td>\n",
       "      <td>23380</td>\n",
       "      <td>973</td>\n",
       "      <td>963</td>\n",
       "      <td>18837</td>\n",
       "      <td>16.303879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Logistic_Regression</td>\n",
       "      <td>Test</td>\n",
       "      <td>0.946100</td>\n",
       "      <td>0.943040</td>\n",
       "      <td>0.936373</td>\n",
       "      <td>0.939695</td>\n",
       "      <td>3319</td>\n",
       "      <td>160</td>\n",
       "      <td>180</td>\n",
       "      <td>2649</td>\n",
       "      <td>16.303879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Passive_Aggressive</td>\n",
       "      <td>Train</td>\n",
       "      <td>0.999932</td>\n",
       "      <td>0.999949</td>\n",
       "      <td>0.999899</td>\n",
       "      <td>0.999924</td>\n",
       "      <td>24352</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>19798</td>\n",
       "      <td>15.580786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Passive_Aggressive</td>\n",
       "      <td>Test</td>\n",
       "      <td>0.939601</td>\n",
       "      <td>0.931594</td>\n",
       "      <td>0.933899</td>\n",
       "      <td>0.932745</td>\n",
       "      <td>3285</td>\n",
       "      <td>194</td>\n",
       "      <td>187</td>\n",
       "      <td>2642</td>\n",
       "      <td>15.580786</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Model Dataset  Accuracy  Precision    Recall        F1  \\\n",
       "0  Logistic_Regression   Train  0.956152   0.950883  0.951364  0.951123   \n",
       "1  Logistic_Regression    Test  0.946100   0.943040  0.936373  0.939695   \n",
       "2   Passive_Aggressive   Train  0.999932   0.999949  0.999899  0.999924   \n",
       "3   Passive_Aggressive    Test  0.939601   0.931594  0.933899  0.932745   \n",
       "\n",
       "      tn   fp   fn     tp  Training_Time  \n",
       "0  23380  973  963  18837      16.303879  \n",
       "1   3319  160  180   2649      16.303879  \n",
       "2  24352    1    2  19798      15.580786  \n",
       "3   3285  194  187   2642      15.580786  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression, PassiveAggressiveClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "from sklearn.pipeline import make_pipeline\n",
    "import joblib\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from lightgbm import LGBMClassifier\n",
    "from time import time\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=10000)\n",
    "\n",
    "\n",
    "models = {\n",
    "    'Logistic_Regression': LogisticRegression(),\n",
    "    'Passive_Aggressive': PassiveAggressiveClassifier(),\n",
    "    'Multinomial_NB': MultinomialNB(),\n",
    "    'K-Nearest_Neighbors': KNeighborsClassifier(),\n",
    "    'Support Vector Machine': SVC(),\n",
    "    'Decsion_Tree': DecisionTreeClassifier(),\n",
    "    'Random_Forest': RandomForestClassifier(),\n",
    "    'Gradient_Boosting': GradientBoostingClassifier(),\n",
    "    'LightGBM': LGBMClassifier(),\n",
    "\n",
    "}\n",
    "evaluation_results = []\n",
    "root_path = Path().resolve().parent\n",
    "\n",
    "train  = pd.read_parquet(f\"{root_path}/data/model_training/train_df.parquet\")\n",
    "test = pd.read_parquet(f\"{root_path}/data/model_training/test_df.parquet\")\n",
    "\n",
    "X_train = train[\"preprocessed_text\"].tolist()\n",
    "y_train = train[\"label\"].tolist()\n",
    "X_valid = test[\"preprocessed_text\"].tolist()\n",
    "y_valid = test[\"label\"].tolist()\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    start_time = time()\n",
    "    print(f'Training {model_name}...')\n",
    "    pipeline = make_pipeline(tfidf_vectorizer, model)\n",
    "    pipeline.fit(X_train, y_train)\n",
    "\n",
    "    joblib.dump(pipeline, f'{root_path}/models/sklearn_models/{model_name}_model.pkl')\n",
    "\n",
    "    # Evaluate on training data\n",
    "    train_pred = pipeline.predict(X_train)\n",
    "    accuracy_train = accuracy_score(y_train, train_pred)\n",
    "    precision_train = precision_score(y_train, train_pred, average='binary', zero_division=0)\n",
    "    recall_train = recall_score(y_train, train_pred, average='binary', zero_division=0)\n",
    "    f1_train = f1_score(y_train, train_pred, average='binary', zero_division=0)\n",
    "    cm_train = confusion_matrix(y_train, train_pred)\n",
    "    \n",
    "    # Evaluate on validation data\n",
    "    test_pred = pipeline.predict(X_valid)\n",
    "    accuracy_test = accuracy_score(y_valid, test_pred)\n",
    "    precision_test = precision_score(y_valid, test_pred, average='binary', zero_division=0)\n",
    "    recall_test = recall_score(y_valid, test_pred, average='binary', zero_division=0)\n",
    "    f1_test = f1_score(y_valid, test_pred, average='binary', zero_division=0)\n",
    "    cm_test = confusion_matrix(y_valid, test_pred)\n",
    "    \n",
    "    training_time = time() - start_time\n",
    "    \n",
    "    tn, fp, fn, tp = cm_train.ravel()\n",
    "    evaluation_results.append({\n",
    "        'Model': model_name,\n",
    "        'Dataset': 'Train',\n",
    "        'Accuracy': accuracy_train,\n",
    "        'Precision': precision_train,\n",
    "        'Recall': recall_train,\n",
    "        'F1': f1_train,\n",
    "        'tn': tn,\n",
    "        'fp': fp,\n",
    "        'fn': fn,\n",
    "        'tp': tp,\n",
    "        'Training_Time': training_time\n",
    "    })\n",
    "    \n",
    "    tn, fp, fn, tp = cm_test.ravel()\n",
    "    evaluation_results.append({\n",
    "        'Model': model_name,\n",
    "        'Dataset': 'Test',\n",
    "        'Accuracy': accuracy_test,\n",
    "        'Precision': precision_test,\n",
    "        'Recall': recall_test,\n",
    "        'F1': f1_test,\n",
    "        'tn': tn,\n",
    "        'fp': fp,\n",
    "        'fn': fn,\n",
    "        'tp': tp,\n",
    "        'Training_Time': training_time\n",
    "    })\n",
    "    print(f'{model_name} accuracy: {accuracy_test:.4f}')\n",
    "    print(f'Precision: {precision_test:.4f}, Recall: {recall_test:.4f}, F1 Score: {f1_test:.4f}')\n",
    "\n",
    "evaluation_df = pd.DataFrame(evaluation_results)\n",
    "evaluation_df.to_parquet(f\"{root_path}/data/model_evaluation/sklearn_models_evaluation.parquet\", index=False)\n",
    "display(evaluation_df)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
