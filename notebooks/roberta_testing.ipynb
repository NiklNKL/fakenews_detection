{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 12854713121253401334\n",
      "xla_global_id: -1\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 5763592192\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 17031650478744042123\n",
      "physical_device_desc: \"device: 0, name: NVIDIA GeForce RTX 2080 SUPER, pci bus id: 0000:0b:00.0, compute capability: 7.5\"\n",
      "xla_global_id: 416903419\n",
      "]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1737574193.471733    3414 gpu_device.cc:2022] Created device /device:GPU:0 with 5496 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2080 SUPER, pci bus id: 0000:0b:00.0, compute capability: 7.5\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>preprocessed_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>house dem aide comey letter jason chaffetz twe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>flynn hillary clinton big woman campus breitba...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>truth fire truth fire october tension intellig...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>civilian kill single airstrike identify video ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>iranian woman jail fictional unpublished story...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20795</th>\n",
       "      <td>0</td>\n",
       "      <td>rapper t trump poster child white supremacy ra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20796</th>\n",
       "      <td>0</td>\n",
       "      <td>n f l playoff schedule matchup odd new york ti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20797</th>\n",
       "      <td>0</td>\n",
       "      <td>macys receive takeover approach hudsons bay ne...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20798</th>\n",
       "      <td>1</td>\n",
       "      <td>nato russia hold parallel exercise balkans nat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20799</th>\n",
       "      <td>1</td>\n",
       "      <td>f alive david swanson author activist journali...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20203 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       label                                  preprocessed_text\n",
       "0          1  house dem aide comey letter jason chaffetz twe...\n",
       "1          0  flynn hillary clinton big woman campus breitba...\n",
       "2          1  truth fire truth fire october tension intellig...\n",
       "3          1  civilian kill single airstrike identify video ...\n",
       "4          1  iranian woman jail fictional unpublished story...\n",
       "...      ...                                                ...\n",
       "20795      0  rapper t trump poster child white supremacy ra...\n",
       "20796      0  n f l playoff schedule matchup odd new york ti...\n",
       "20797      0  macys receive takeover approach hudsons bay ne...\n",
       "20798      1  nato russia hold parallel exercise balkans nat...\n",
       "20799      1  f alive david swanson author activist journali...\n",
       "\n",
       "[20203 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_parquet('../data/train_preprocessed_small.parquet')\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 14142\n",
      "Validation size: 4242\n",
      "Test size: 1819\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(df[\"preprocessed_text\"], df['label'], test_size=0.3, random_state=42)\n",
    "\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_val, y_val, test_size=0.3, random_state=42)\n",
    "\n",
    "print(f\"Train size: {len(X_train)}\")\n",
    "print(f\"Validation size: {len(X_val)}\")\n",
    "print(f\"Test size: {len(X_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nikl/programming/fhdw/knowledge_engineering_ausarbeitung/fakenews_detection/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-01-21 14:28:24.771656: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-01-21 14:28:24.863910: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-01-21 14:28:24.951399: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1737466105.023110   11613 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1737466105.044146   11613 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-01-21 14:28:25.222816: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "W0000 00:00:1737466108.724340   11613 gpu_device.cc:2344] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFRobertaForSequenceClassification: ['roberta.embeddings.position_ids']\n",
      "- This IS expected if you are initializing TFRobertaForSequenceClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFRobertaForSequenceClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights or buffers of the TF 2.0 model TFRobertaForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Base models loaded\n"
     ]
    }
   ],
   "source": [
    "from transformers import RobertaTokenizer, TFRobertaForSequenceClassification, AdamW\n",
    "import tensorflow as tf\n",
    "\n",
    "roberta_tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\", do_lower_case=True)\n",
    "roberta_model = TFRobertaForSequenceClassification.from_pretrained(\"roberta-base\", #RoBERTa base model\n",
    "                                                                    num_labels = 2,  #number of output labels - 0,1 (binary classification)\n",
    "                                                                    output_attentions = False,  #model doesnt return attention weights\n",
    "                                                                    output_hidden_states = False #model doesnt return hidden states\n",
    "                                                                )\n",
    "\n",
    "\n",
    "# roberta_model.cuda()\n",
    "print(' Base models loaded') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16677    james stavridis retire admiral vet hillary cli...\n",
       "14013    state swat team drill prep backlash steal elec...\n",
       "14575    george michaels death meet disbelief celebrity...\n",
       "5629     build cute bee hotel help population event chr...\n",
       "8275     new comment feature add comment leave reply cl...\n",
       "                               ...                        \n",
       "11625    chaiwali indian restaurant feel like home new ...\n",
       "12329    recipe mouth watering cauliflower coconut oil ...\n",
       "5553     las cajetillas de tabaco emitir n m sica de me...\n",
       "885      open border group gird h b fight open border l...\n",
       "16251    eighty wealthy new yorker ask state government...\n",
       "Name: preprocessed_text, Length: 14142, dtype: object"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized RoBERT:  ['j', 'ames', 'Ä st', 'av', 'rid', 'is', 'Ä retire', 'Ä adm', 'iral', 'Ä vet', 'Ä hill', 'ary', 'Ä cl', 'inton', 'Ä run', 'Ä mate', 'Ä new', 'Ä y', 'ork', 'Ä times', 'Ä hill', 'ary', 'Ä cl', 'inton', 'Ä campaign', 'Ä vet', 'Ä j', 'ame', 'Ä g', 'Ä st', 'av', 'rid', 'is', 'Ä retire', 'Ä navy', 'Ä adm', 'iral', 'Ä serve', 'Ä th', 'Ä supreme', 'Ä ally', 'Ä commander', 'Ä n', 'ato', 'Ä possible', 'Ä running', 'Ä mate', 'Ä accord', 'Ä person', 'Ä knowledge', 'Ä vetting', 'Ä process', 'Ä close', 'Ä m', 'rs', 'Ä cl', 'inton', 'Ä secretary', 'Ä state', 'Ä likely', 'Ä military', 'Ä experience', 'Ä short', 'list', 'Ä m', 'r', 'Ä st', 'av', 'rid', 'is', 'Ä currently', 'Ä dean', 'Ä f', 'letcher', 'Ä school', 'Ä tu', 'fts', 'Ä university', 'Ä fit', 'Ä description', 'Ä year', 'Ä nat', 'os', 'Ä supreme', 'Ä ally', 'Ä commander', 'Ä oversee', 'Ä operation', 'Ä middle', 'Ä east', 'Ä af', 'ghan', 'istan', 'Ä lib', 'ya', 'Ä sy', 'ria', 'Ä balk', 'an', 'Ä piracy', 'Ä coast', 'Ä af', 'rica', 'Ä cl', 'inton', 'Ä campaign', 'Ä decline', 'Ä request', 'Ä comment', 'Ä m', 'r', 'Ä st', 'av', 'rid', 'is', 'Ä decline', 'Ä comment', 'Ä refer', 'Ä campaign', 'Ä person', 'Ä knowledge', 'Ä vetting', 'Ä speak', 'Ä anonymously', 'Ä sensitive', 'Ä nature', 'Ä process', 'Ä m', 'r', 'Ä st', 'av', 'rid', 'is', 'Ä investigate', 'Ä improperly', 'Ä military', 'Ä aircraft', 'Ä fly', 'Ä wife', 'Ä exclusive', 'Ä party', 'Ä burg', 'undy', 'Ä fr', 'ance', 'Ä win', 'emaker', 'Ä later', 'Ä clear', 'Ä misconduct', 'Ä long', 'Ä pent', 'agon', 'Ä investigation', 'Ä travel', 'Ä expense', 'Ä include', 'Ä trip', 'Ä wife', 'Ä daughter', 'Ä mother', 'Ä pent', 'agon', 'Ä inspector', 'Ä general', 'Ä report', 'Ä ultimately', 'Ä conclude', 'Ä fail', 'Ä exercise', 'Ä sufficient', 'Ä oversight', 'Ä staff', 'Ä member', 'Ä book', 'keeping', 'Ä mistake', 'Ä don', 'ald', 'Ä j', 'Ä trump', 'Ä certain', 'Ä appeal', 'Ä select', 'Ä military', 'Ä experience', 'Ä running', 'Ä mate', 'Ä consider', 'Ä retire', 'Ä l', 't', 'Ä gen', 'Ä m', 'ichael', 'Ä fly', 'nn', 'Ä interview', 'Ä ab', 'c', 'Ä news', 'Ä weekend', 'Ä m', 'r', 'Ä fly', 'nn', 'Ä lifelong', 'Ä democrat', 'Ä stumble', 'Ä offer', 'Ä support', 'Ä abortion', 'Ä right', 'Ä view', 'Ä lock', 'Ä step', 'Ä republican', 'Ä base', 'Ä orthodoxy', 'Ä phone', 'Ä interview', 'Ä new', 'Ä y', 'ork', 'Ä times', 'Ä t', 'uesday', 'Ä m', 'r', 'Ä trump', 'Ä m', 'r', 'Ä fly', 'nn', 'Ä patriot', 'Ä m', 'r', 'Ä trump', 'Ä continue', 'Ä public', 'Ä audition', 'Ä process', 'Ä possible', 'Ä running', 'Ä mate', 'Ä t', 'uesday', 'Ä night', 'Ä set', 'Ä appear', 'Ä campaign', 'Ä rally', 'Ä ind', 'iana', 'Ä m', 'ike', 'Ä p', 'ence', 'Ä state', 'Ä governor']\n"
     ]
    }
   ],
   "source": [
    "# Split the sentence into tokens -RoBERTa\n",
    "print('Tokenized RoBERT: ', roberta_tokenizer.tokenize(X_train.iloc[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token IDs RoBERTa:  [267, 12336, 1690, 1469, 10505, 354, 7865, 20087, 24811, 9087, 9910, 1766, 3741, 10528, 422, 12563, 92, 1423, 9657, 498, 9910, 1766, 3741, 10528, 637, 9087, 1236, 4344, 821, 1690, 1469, 10505, 354, 7865, 13504, 20087, 24811, 1807, 3553, 15835, 7564, 8501, 295, 3938, 678, 878, 12563, 10170, 621, 2655, 23371, 609, 593, 475, 4926, 3741, 10528, 2971, 194, 533, 831, 676, 765, 8458, 475, 338, 1690, 1469, 10505, 354, 855, 19955, 856, 42795, 334, 13145, 21811, 2737, 2564, 8194, 76, 23577, 366, 15835, 7564, 8501, 13752, 2513, 1692, 3017, 9724, 5158, 7566, 21748, 2636, 13550, 6374, 31110, 260, 33551, 3673, 9724, 14962, 3741, 10528, 637, 2991, 2069, 1129, 475, 338, 1690, 1469, 10505, 354, 2991, 1129, 9115, 637, 621, 2655, 23371, 1994, 20797, 5685, 2574, 609, 475, 338, 1690, 1469, 10505, 354, 4830, 21559, 831, 3054, 3598, 1141, 5451, 537, 18521, 29868, 6664, 2389, 339, 18362, 423, 699, 6046, 251, 8027, 11408, 803, 1504, 5623, 680, 1805, 1141, 1354, 985, 8027, 11408, 14253, 937, 266, 3284, 10992, 5998, 3325, 7719, 9233, 813, 919, 1040, 12609, 5021, 218, 5618, 1236, 20125, 1402, 2868, 5163, 831, 676, 878, 12563, 1701, 7865, 784, 90, 12358, 475, 25554, 3598, 15688, 1194, 4091, 438, 340, 983, 475, 338, 3598, 15688, 15353, 26232, 30282, 904, 323, 6428, 235, 1217, 7014, 1149, 37958, 1542, 43446, 1028, 1194, 92, 1423, 9657, 498, 326, 47478, 475, 338, 20125, 475, 338, 3598, 15688, 36409, 475, 338, 20125, 535, 285, 17476, 609, 678, 878, 12563, 326, 47478, 363, 278, 2082, 637, 2669, 9473, 8878, 475, 4348, 181, 4086, 194, 2318]\n"
     ]
    }
   ],
   "source": [
    "# Mapping tokens to token IDs - RoBERTa\n",
    "print('Token IDs RoBERTa: ', roberta_tokenizer.convert_tokens_to_ids(roberta_tokenizer.tokenize(X_train.iloc[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_encoded_roberta = roberta_tokenizer(X_train.to_list(), padding=True, truncation=True, max_length=512, return_tensors=\"tf\")\n",
    "val_encoded_roberta = roberta_tokenizer(X_val.to_list(), padding=True, truncation=True, max_length=512, return_tensors=\"tf\")\n",
    "test_encoded_roberta = roberta_tokenizer(X_test.to_list(), padding=True, truncation=True, max_length=512, return_tensors=\"tf\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = tf.convert_to_tensor(y_train.to_list(), dtype=tf.int32)\n",
    "y_val = tf.convert_to_tensor(y_val.to_list(), dtype=tf.int32)\n",
    "y_test = tf.convert_to_tensor(y_test.to_list(), dtype=tf.int32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    {\"input_ids\": train_encoded_roberta[\"input_ids\"], \n",
    "     \"attention_mask\": train_encoded_roberta[\"attention_mask\"]}, \n",
    "    y_train\n",
    ")).batch(32)\n",
    "\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    {\"input_ids\": val_encoded_roberta[\"input_ids\"], \n",
    "     \"attention_mask\": val_encoded_roberta[\"attention_mask\"]}, \n",
    "    y_val\n",
    ")).batch(32)\n",
    "\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    {\"input_ids\": test_encoded_roberta[\"input_ids\"], \n",
    "     \"attention_mask\": test_encoded_roberta[\"attention_mask\"]}, \n",
    "    y_test\n",
    ")).batch(32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-21 14:53:04.130540: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-01-21 14:53:04.133357: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-01-21 14:53:04.142423: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1737467584.157127    1095 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1737467584.161536    1095 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-01-21 14:53:04.176183: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1737467586.433070    1095 gpu_device.cc:2344] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(\"Num GPUs Available:\", len(tf.config.list_physical_devices('GPU')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = train_dataset.batch(8)  # Reduce batch size\n",
    "val_dataset = val_dataset.batch(8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.mixed_precision import set_global_policy\n",
    "set_global_policy(\"mixed_float16\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Compile the model\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=5e-5)\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "metrics = [\"accuracy\"]\n",
    "\n",
    "roberta_model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n",
    "\n",
    "# Train the model\n",
    "roberta_model.fit(train_dataset, validation_data=val_dataset, epochs=3)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
