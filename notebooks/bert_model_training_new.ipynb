{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nikl/programming/fhdw/knowledge_engineering_ausarbeitung/fakenews_detection/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from custom_utils import load_and_concatenate_parquet_files\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import AutoTokenizer, AutoConfig, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "from peft import get_peft_model, LoraConfig\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>label_names</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>donald trump respond mockery fake swedish atta...</td>\n",
       "      <td>1</td>\n",
       "      <td>real</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>tweetwavethis time true pantstweetwave anthony...</td>\n",
       "      <td>1</td>\n",
       "      <td>real</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>rubio prospect trump president worrisome reute...</td>\n",
       "      <td>0</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>trump lifts cyber command status boost cyber d...</td>\n",
       "      <td>0</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>big republican lie economy tear apart minute v...</td>\n",
       "      <td>1</td>\n",
       "      <td>real</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63116</th>\n",
       "      <td>half briton want stay eu polledinburgh reuters...</td>\n",
       "      <td>0</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63117</th>\n",
       "      <td>bill hillary clinton inc sale right pricein sp...</td>\n",
       "      <td>1</td>\n",
       "      <td>real</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63118</th>\n",
       "      <td>orlando gunman shoot time autopsy find new yor...</td>\n",
       "      <td>0</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63119</th>\n",
       "      <td>lethal gap supreme court handle death penalty ...</td>\n",
       "      <td>0</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63120</th>\n",
       "      <td>poll world overwhelmingly love president obama...</td>\n",
       "      <td>1</td>\n",
       "      <td>real</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>63121 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  label label_names\n",
       "0      donald trump respond mockery fake swedish atta...      1        real\n",
       "1      tweetwavethis time true pantstweetwave anthony...      1        real\n",
       "2      rubio prospect trump president worrisome reute...      0        fake\n",
       "3      trump lifts cyber command status boost cyber d...      0        fake\n",
       "4      big republican lie economy tear apart minute v...      1        real\n",
       "...                                                  ...    ...         ...\n",
       "63116  half briton want stay eu polledinburgh reuters...      0        fake\n",
       "63117  bill hillary clinton inc sale right pricein sp...      1        real\n",
       "63118  orlando gunman shoot time autopsy find new yor...      0        fake\n",
       "63119  lethal gap supreme court handle death penalty ...      0        fake\n",
       "63120  poll world overwhelmingly love president obama...      1        real\n",
       "\n",
       "[63121 rows x 3 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df  = load_and_concatenate_parquet_files('data/preprocessed_big_training_df')\n",
    "\n",
    "df = df.rename(columns={'preprocessed_text': 'text'})\n",
    "df[\"label_names\"] = df[\"label\"].apply(lambda x: \"real\" if x == 1 else \"fake\")\n",
    "display(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((44184, 3), (12624, 3), (6313, 3))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train,test      = train_test_split(df,test_size=0.3,stratify=df['label'])\n",
    "test,validation = train_test_split(test,test_size=1/3,stratify=test['label'])\n",
    "train.shape, test.shape, validation.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label', 'label_names'],\n",
       "        num_rows: 44184\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label', 'label_names'],\n",
       "        num_rows: 12624\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text', 'label', 'label_names'],\n",
       "        num_rows: 6313\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = DatasetDict(\n",
    "    {'train':Dataset.from_pandas(train,preserve_index=False),\n",
    "     'test':Dataset.from_pandas(test,preserve_index=False),\n",
    "     'validation': Dataset.from_pandas(validation,preserve_index=False)\n",
    "     }    \n",
    ")\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'real': 1, 'fake': 0}, {1: 'real', 0: 'fake'})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label2id = {x['label_names']:x['label'] for x in dataset['train']}\n",
    "id2label = {v:k for k,v in label2id.items()}\n",
    "label2id, id2label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DebertaV2Tokenizer\n",
    "# Load dataset\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load the evaluation metric\n",
    "accuracy_metric = evaluate.load(\"accuracy\")\n",
    "f1_metric = evaluate.load(\"f1\")\n",
    "precision_metric = evaluate.load(\"precision\")\n",
    "recall_metric = evaluate.load(\"recall\")\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    logits, labels = pred\n",
    "    predictions = torch.argmax(torch.tensor(logits), dim=-1)\n",
    "    result = {}\n",
    "    for metric in [accuracy_metric, f1_metric, precision_metric, recall_metric]:\n",
    "        result.update(metric.compute(predictions=predictions, references=labels))\n",
    "    return result\n",
    "\n",
    "# Fine-tuning function\n",
    "def fine_tune_model(model_ckpt, dataset, output_dir, training_batch_size=32):\n",
    "    print(f\"Using Model: {model_ckpt}\")\n",
    "    print(f\"Tokenizing Data\")\n",
    "    # Tokenizer and dataset preparation\n",
    "    if model_ckpt == \"microsoft/deberta-v3-base\":\n",
    "        tokenizer = DebertaV2Tokenizer.from_pretrained(model_ckpt, use_fast=True)\n",
    "    else:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_ckpt) \n",
    "    def tokenize_and_format(batch):\n",
    "        tokens = tokenizer(batch['text'], padding=True, truncation=True)\n",
    "        # Convert to PyTorch tensors and move to the correct device\n",
    "        tokens = {key: torch.tensor(val).to(device) for key, val in tokens.items()}\n",
    "        tokens['labels'] = torch.tensor(batch['label']).to(device)\n",
    "        return tokens\n",
    "    tokenized_dataset = dataset.map(tokenize_and_format, batched=True)\n",
    "\n",
    "    # Config and model\n",
    "    config = AutoConfig.from_pretrained(model_ckpt, num_labels=2)  # Adjust num_labels if needed\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_ckpt, config=config).to(device)\n",
    "    \n",
    "    if model_ckpt == \"distilbert-base-uncased\":\n",
    "        target_modules = [\"q_lin\", \"k_lin\",\"v_lin\"]\n",
    "    elif model_ckpt == \"microsoft/deberta-v3-base\":\n",
    "        target_modules = None\n",
    "    else:\n",
    "        target_modules = [\"query\", \"value\"]\n",
    "\n",
    "    \n",
    "    # PEFT: LoRA\n",
    "    peft_config = LoraConfig(\n",
    "        task_type=\"SEQ_CLS\",\n",
    "        r=8,  # Smaller rank to reduce file size\n",
    "        lora_alpha=32,  # Adjust scaling factor\n",
    "        lora_dropout=0.1,\n",
    "        target_modules=target_modules\n",
    "    )\n",
    "    model = get_peft_model(model, peft_config)\n",
    "    \n",
    "    # Training arguments\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        num_train_epochs=5,  # Adjust based on needs\n",
    "        per_device_train_batch_size=training_batch_size,\n",
    "        per_device_eval_batch_size=training_batch_size,\n",
    "        learning_rate=2e-5,\n",
    "        weight_decay=0.01,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        logging_dir=f\"{output_dir}/logs\",\n",
    "        save_total_limit=4,  # Limit checkpoints\n",
    "        fp16=True,  # Mixed precision for speed\n",
    "        logging_steps=50,\n",
    "        report_to=\"tensorboard\",\n",
    "        lr_scheduler_type=\"linear\",\n",
    "        warmup_steps=500,\n",
    "    )\n",
    "    \n",
    "    # Trainer setup\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_dataset[\"train\"],\n",
    "        eval_dataset=tokenized_dataset[\"test\"],\n",
    "        compute_metrics=compute_metrics\n",
    "    )\n",
    "    print(\"Starting Training\")\n",
    "    # Train\n",
    "    trainer.train()\n",
    "    test_results = trainer.evaluate(eval_dataset=dataset[\"test\"])\n",
    "    print(f\"Test Results: {test_results}\")\n",
    "    # Save LoRA-only model\n",
    "    model.save_pretrained(output_dir)\n",
    "    tokenizer.save_pretrained(output_dir)\n",
    "    print(f\"Finished training {model_ckpt}. Model saved to {output_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Model: microsoft/deberta-v3-base\n",
      "Tokenizing Data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 0/44184 [00:00<?, ? examples/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 44184/44184 [02:10<00:00, 338.96 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12624/12624 [00:38<00:00, 325.17 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6313/6313 [00:19<00:00, 323.97 examples/s]\n",
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Target modules {'transformer.layer.*.attention.query', 'transformer.layer.*.attention.dense', 'transformer.layer.*.attention.key', 'transformer.layer.*.attention.value'} not found in the base model. Please check the target modules and try again.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 16\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(output_dir):\n\u001b[1;32m     14\u001b[0m     os\u001b[38;5;241m.\u001b[39mmakedirs(output_dir)\n\u001b[0;32m---> 16\u001b[0m \u001b[43mfine_tune_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_ckpt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[6], line 61\u001b[0m, in \u001b[0;36mfine_tune_model\u001b[0;34m(model_ckpt, dataset, output_dir, training_batch_size)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;66;03m# PEFT: LoRA\u001b[39;00m\n\u001b[1;32m     54\u001b[0m peft_config \u001b[38;5;241m=\u001b[39m LoraConfig(\n\u001b[1;32m     55\u001b[0m     task_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSEQ_CLS\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     56\u001b[0m     r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m,  \u001b[38;5;66;03m# Smaller rank to reduce file size\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     59\u001b[0m     target_modules\u001b[38;5;241m=\u001b[39mtarget_modules\n\u001b[1;32m     60\u001b[0m )\n\u001b[0;32m---> 61\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mget_peft_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpeft_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;66;03m# Training arguments\u001b[39;00m\n\u001b[1;32m     64\u001b[0m training_args \u001b[38;5;241m=\u001b[39m TrainingArguments(\n\u001b[1;32m     65\u001b[0m     output_dir\u001b[38;5;241m=\u001b[39moutput_dir,\n\u001b[1;32m     66\u001b[0m     num_train_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m,  \u001b[38;5;66;03m# Adjust based on needs\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     79\u001b[0m     warmup_steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m500\u001b[39m,\n\u001b[1;32m     80\u001b[0m )\n",
      "File \u001b[0;32m~/programming/fhdw/knowledge_engineering_ausarbeitung/fakenews_detection/.venv/lib/python3.10/site-packages/peft/mapping.py:222\u001b[0m, in \u001b[0;36mget_peft_model\u001b[0;34m(model, peft_config, adapter_name, mixed, autocast_adapter_dtype, revision, low_cpu_mem_usage)\u001b[0m\n\u001b[1;32m    220\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m peft_config\u001b[38;5;241m.\u001b[39mis_prompt_learning:\n\u001b[1;32m    221\u001b[0m     peft_config \u001b[38;5;241m=\u001b[39m _prepare_prompt_learning_config(peft_config, model_config)\n\u001b[0;32m--> 222\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mMODEL_TYPE_TO_PEFT_MODEL_MAPPING\u001b[49m\u001b[43m[\u001b[49m\u001b[43mpeft_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtask_type\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    223\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    224\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpeft_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    225\u001b[0m \u001b[43m    \u001b[49m\u001b[43madapter_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madapter_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    226\u001b[0m \u001b[43m    \u001b[49m\u001b[43mautocast_adapter_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mautocast_adapter_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    227\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    228\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/programming/fhdw/knowledge_engineering_ausarbeitung/fakenews_detection/.venv/lib/python3.10/site-packages/peft/peft_model.py:1449\u001b[0m, in \u001b[0;36mPeftModelForSequenceClassification.__init__\u001b[0;34m(self, model, peft_config, adapter_name, **kwargs)\u001b[0m\n\u001b[1;32m   1446\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m   1447\u001b[0m     \u001b[38;5;28mself\u001b[39m, model: torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mModule, peft_config: PeftConfig, adapter_name: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdefault\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m   1448\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1449\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpeft_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madapter_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1451\u001b[0m     classifier_module_names \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclassifier\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscore\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1452\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodules_to_save \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/programming/fhdw/knowledge_engineering_ausarbeitung/fakenews_detection/.venv/lib/python3.10/site-packages/peft/peft_model.py:176\u001b[0m, in \u001b[0;36mPeftModel.__init__\u001b[0;34m(self, model, peft_config, adapter_name, autocast_adapter_dtype, low_cpu_mem_usage)\u001b[0m\n\u001b[1;32m    174\u001b[0m     ctx \u001b[38;5;241m=\u001b[39m init_empty_weights \u001b[38;5;28;01mif\u001b[39;00m low_cpu_mem_usage \u001b[38;5;28;01melse\u001b[39;00m nullcontext\n\u001b[1;32m    175\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx():\n\u001b[0;32m--> 176\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43madapter_name\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpeft_config\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madapter_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    177\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_additional_trainable_modules(peft_config, adapter_name)\n\u001b[1;32m    179\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_model, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_cast_adapter_dtype\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/programming/fhdw/knowledge_engineering_ausarbeitung/fakenews_detection/.venv/lib/python3.10/site-packages/peft/tuners/lora/model.py:141\u001b[0m, in \u001b[0;36mLoraModel.__init__\u001b[0;34m(self, model, config, adapter_name, low_cpu_mem_usage)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, model, config, adapter_name, low_cpu_mem_usage: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 141\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madapter_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/programming/fhdw/knowledge_engineering_ausarbeitung/fakenews_detection/.venv/lib/python3.10/site-packages/peft/tuners/tuners_utils.py:184\u001b[0m, in \u001b[0;36mBaseTuner.__init__\u001b[0;34m(self, model, peft_config, adapter_name, low_cpu_mem_usage)\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pre_injection_hook(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpeft_config[adapter_name], adapter_name)\n\u001b[1;32m    183\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m peft_config \u001b[38;5;241m!=\u001b[39m PeftType\u001b[38;5;241m.\u001b[39mXLORA \u001b[38;5;129;01mor\u001b[39;00m peft_config[adapter_name] \u001b[38;5;241m!=\u001b[39m PeftType\u001b[38;5;241m.\u001b[39mXLORA:\n\u001b[0;32m--> 184\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minject_adapter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madapter_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;66;03m# Copy the peft_config in the injected model.\u001b[39;00m\n\u001b[1;32m    187\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mpeft_config \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpeft_config\n",
      "File \u001b[0;32m~/programming/fhdw/knowledge_engineering_ausarbeitung/fakenews_detection/.venv/lib/python3.10/site-packages/peft/tuners/tuners_utils.py:520\u001b[0m, in \u001b[0;36mBaseTuner.inject_adapter\u001b[0;34m(self, model, adapter_name, autocast_adapter_dtype, low_cpu_mem_usage)\u001b[0m\n\u001b[1;32m    518\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m peft_config\u001b[38;5;241m.\u001b[39mlayers_pattern \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    519\u001b[0m         error_msg \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m You also specified \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlayers_pattern\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpeft_config\u001b[38;5;241m.\u001b[39mlayers_pattern\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 520\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(error_msg)\n\u001b[1;32m    521\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    522\u001b[0m     \u001b[38;5;66;03m# Some modules did not match and some matched but were excluded\u001b[39;00m\n\u001b[1;32m    523\u001b[0m     error_msg \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    524\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo modules were targeted for adaptation. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    525\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis might be caused by a combination of mismatched target modules and excluded modules. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    526\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease check your `target_modules` and `exclude_modules` configuration.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    527\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Target modules {'transformer.layer.*.attention.query', 'transformer.layer.*.attention.dense', 'transformer.layer.*.attention.key', 'transformer.layer.*.attention.value'} not found in the base model. Please check the target modules and try again."
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "model_checkpoints = [\n",
    "    # \"bert-base-uncased\",\n",
    "    # \"distilbert-base-uncased\",\n",
    "    # \"roberta-base\",\n",
    "    \"microsoft/deberta-v3-base\"\n",
    "]\n",
    "\n",
    "# Iterate over models\n",
    "for model_ckpt in model_checkpoints:\n",
    "    output_dir = f\"models/{model_ckpt.replace('/', '_')}\"\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "        \n",
    "    fine_tune_model(model_ckpt, dataset, output_dir, training_batch_size=32)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
