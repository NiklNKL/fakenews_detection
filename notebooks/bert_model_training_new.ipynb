{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/developing_nacho/fhdw/knowledge_engineering/fakenews_detection/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-01-26 00:20:33.051711: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-01-26 00:20:33.170334: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2025-01-26 00:20:33.170347: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "from custom_utils import load_and_concatenate_parquet_files\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import AutoTokenizer, AutoConfig, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "from peft import get_peft_model, LoraConfig\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>label_names</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>donald trump respond mockery fake swedish atta...</td>\n",
       "      <td>1</td>\n",
       "      <td>real</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>tweetwavethis time true pantstweetwave anthony...</td>\n",
       "      <td>1</td>\n",
       "      <td>real</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>rubio prospect trump president worrisome reute...</td>\n",
       "      <td>0</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>trump lifts cyber command status boost cyber d...</td>\n",
       "      <td>0</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>big republican lie economy tear apart minute v...</td>\n",
       "      <td>1</td>\n",
       "      <td>real</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63116</th>\n",
       "      <td>half briton want stay eu polledinburgh reuters...</td>\n",
       "      <td>0</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63117</th>\n",
       "      <td>bill hillary clinton inc sale right pricein sp...</td>\n",
       "      <td>1</td>\n",
       "      <td>real</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63118</th>\n",
       "      <td>orlando gunman shoot time autopsy find new yor...</td>\n",
       "      <td>0</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63119</th>\n",
       "      <td>lethal gap supreme court handle death penalty ...</td>\n",
       "      <td>0</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63120</th>\n",
       "      <td>poll world overwhelmingly love president obama...</td>\n",
       "      <td>1</td>\n",
       "      <td>real</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>63121 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  label label_names\n",
       "0      donald trump respond mockery fake swedish atta...      1        real\n",
       "1      tweetwavethis time true pantstweetwave anthony...      1        real\n",
       "2      rubio prospect trump president worrisome reute...      0        fake\n",
       "3      trump lifts cyber command status boost cyber d...      0        fake\n",
       "4      big republican lie economy tear apart minute v...      1        real\n",
       "...                                                  ...    ...         ...\n",
       "63116  half briton want stay eu polledinburgh reuters...      0        fake\n",
       "63117  bill hillary clinton inc sale right pricein sp...      1        real\n",
       "63118  orlando gunman shoot time autopsy find new yor...      0        fake\n",
       "63119  lethal gap supreme court handle death penalty ...      0        fake\n",
       "63120  poll world overwhelmingly love president obama...      1        real\n",
       "\n",
       "[63121 rows x 3 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df  = load_and_concatenate_parquet_files('data/preprocessed_big_training_df')\n",
    "\n",
    "df = df.rename(columns={'preprocessed_text': 'text'})\n",
    "df[\"label_names\"] = df[\"label\"].apply(lambda x: \"real\" if x == 1 else \"fake\")\n",
    "display(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((44184, 3), (12624, 3), (6313, 3))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train,test      = train_test_split(df,test_size=0.3,stratify=df['label'])\n",
    "test,validation = train_test_split(test,test_size=1/3,stratify=test['label'])\n",
    "train.shape, test.shape, validation.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label', 'label_names'],\n",
       "        num_rows: 44184\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label', 'label_names'],\n",
       "        num_rows: 12624\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text', 'label', 'label_names'],\n",
       "        num_rows: 6313\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = DatasetDict(\n",
    "    {'train':Dataset.from_pandas(train,preserve_index=False),\n",
    "     'test':Dataset.from_pandas(test,preserve_index=False),\n",
    "     'validation': Dataset.from_pandas(validation,preserve_index=False)\n",
    "     }    \n",
    ")\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'fake': 0, 'real': 1}, {0: 'fake', 1: 'real'})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label2id = {x['label_names']:x['label'] for x in dataset['train']}\n",
    "id2label = {v:k for k,v in label2id.items()}\n",
    "label2id, id2label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DebertaV2Tokenizer\n",
    "# Load dataset\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load the evaluation metric\n",
    "accuracy_metric = evaluate.load(\"accuracy\")\n",
    "f1_metric = evaluate.load(\"f1\")\n",
    "precision_metric = evaluate.load(\"precision\")\n",
    "recall_metric = evaluate.load(\"recall\")\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    logits, labels = pred\n",
    "    predictions = torch.argmax(torch.tensor(logits), dim=-1)\n",
    "    result = {}\n",
    "    for metric in [accuracy_metric, f1_metric, precision_metric, recall_metric]:\n",
    "        result.update(metric.compute(predictions=predictions, references=labels))\n",
    "    return result\n",
    "\n",
    "# Fine-tuning function\n",
    "def fine_tune_model(model_ckpt, dataset, output_dir, training_batch_size=32, checkpoint=None, epochs=5):\n",
    "    print(f\"Using Model: {model_ckpt}\")\n",
    "    print(f\"Tokenizing Data\")\n",
    "    # Tokenizer and dataset preparation\n",
    "    if model_ckpt == \"microsoft/deberta-v3-base\":\n",
    "        tokenizer = DebertaV2Tokenizer.from_pretrained(model_ckpt, use_fast=True)\n",
    "    else:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_ckpt) \n",
    "    def tokenize_and_format(batch):\n",
    "        tokens = tokenizer(batch['text'], padding=True, truncation=True)\n",
    "        # Convert to PyTorch tensors and move to the correct device\n",
    "        tokens = {key: torch.tensor(val).to(device) for key, val in tokens.items()}\n",
    "        tokens['labels'] = torch.tensor(batch['label']).to(device)\n",
    "        return tokens\n",
    "    tokenized_dataset = dataset.map(tokenize_and_format, batched=True)\n",
    "\n",
    "    # Config and model\n",
    "    config = AutoConfig.from_pretrained(model_ckpt, num_labels=2)  # Adjust num_labels if needed\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_ckpt, config=config).to(device)\n",
    "        \n",
    "    if model_ckpt == \"distilbert-base-uncased\":\n",
    "        target_modules = [\"q_lin\", \"k_lin\",\"v_lin\"]\n",
    "    elif model_ckpt == \"microsoft/deberta-v3-base\":\n",
    "        target_modules = None\n",
    "    else:\n",
    "        target_modules = [\"query\", \"value\"]\n",
    "\n",
    "    \n",
    "    # PEFT: LoRA\n",
    "    peft_config = LoraConfig(\n",
    "        task_type=\"SEQ_CLS\",\n",
    "        r=8,  # Smaller rank to reduce file size\n",
    "        lora_alpha=32,  # Adjust scaling factor\n",
    "        lora_dropout=0.1,\n",
    "        target_modules=target_modules\n",
    "    )\n",
    "    model = get_peft_model(model, peft_config)\n",
    "    \n",
    "    if checkpoint:\n",
    "        print(f\"Loading LoRA weights from {checkpoint}\")\n",
    "        model.load_state_dict(torch.load(checkpoint), strict=False)\n",
    "\n",
    "    # Training arguments\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        num_train_epochs=epochs,  # Adjust based on needs\n",
    "        per_device_train_batch_size=training_batch_size,\n",
    "        per_device_eval_batch_size=training_batch_size,\n",
    "        learning_rate=2e-5,\n",
    "        weight_decay=0.01,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        logging_dir=f\"{output_dir}/logs\",\n",
    "        save_total_limit=4,  # Limit checkpoints\n",
    "        fp16=True,  # Mixed precision for speed\n",
    "        logging_steps=50,\n",
    "        report_to=\"tensorboard\",\n",
    "        lr_scheduler_type=\"linear\",\n",
    "        warmup_steps=500,\n",
    "    )\n",
    "    \n",
    "    # Trainer setup\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_dataset[\"train\"],\n",
    "        eval_dataset=tokenized_dataset[\"test\"],\n",
    "        compute_metrics=compute_metrics\n",
    "    )\n",
    "    print(\"Starting Training\")\n",
    "    # Train\n",
    "    trainer.train(resume_from_checkpoint=checkpoint != None)\n",
    "    test_results = trainer.evaluate(eval_dataset=tokenized_dataset[\"test\"])\n",
    "    print(f\"Test Results: {test_results}\")\n",
    "    # Save LoRA-only model\n",
    "    model.save_pretrained(output_dir)\n",
    "    tokenizer.save_pretrained(output_dir)\n",
    "    print(f\"Finished training {model_ckpt}. Model saved to {output_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Model: distilbert-base-uncased\n",
      "Tokenizing Data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 44184/44184 [00:07<00:00, 5729.87 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12624/12624 [00:02<00:00, 5909.92 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6313/6313 [00:01<00:00, 5822.06 examples/s]\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipykernel_7794/1991638647.py:60: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(checkpoint), strict=False)\n",
      "/home/developing_nacho/fhdw/knowledge_engineering/fakenews_detection/.venv/lib/python3.10/site-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading LoRA weights from /home/developing_nacho/fhdw/knowledge_engineering/fakenews_detection/models/distilbert-base-uncased/checkpoint-3455/rng_state.pth\n",
      "Starting Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/developing_nacho/fhdw/knowledge_engineering/fakenews_detection/.venv/lib/python3.10/site-packages/transformers/trainer.py:3441: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(os.path.join(checkpoint, OPTIMIZER_NAME), map_location=map_location)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3455' max='691' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3455/691 : < :, Epoch 5/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='198' max='198' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [198/198 00:26]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Results: {'eval_loss': 0.11238894611597061, 'eval_accuracy': 0.9528675538656527, 'eval_f1': 0.9473684210526315, 'eval_precision': 0.949636460365313, 'eval_recall': 0.9451111895517119, 'eval_runtime': 26.4476, 'eval_samples_per_second': 477.321, 'eval_steps_per_second': 7.486, 'epoch': 5.0}\n",
      "Finished training distilbert-base-uncased. Model saved to models/distilbert-base-uncased\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "model_checkpoints = [\n",
    "    \"bert-base-uncased\",\n",
    "    # \"distilbert-base-uncased\",\n",
    "    \"roberta-base\",\n",
    "    \"microsoft/deberta-v3-base\"\n",
    "]\n",
    "\n",
    "# Iterate over models\n",
    "for model_ckpt in model_checkpoints:\n",
    "    output_dir = f\"models/{model_ckpt.replace('/', '_')}\"\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "        \n",
    "    fine_tune_model(model_ckpt, dataset, output_dir, training_batch_size=64, epochs=5)\n",
    "\n",
    "# checkpoint=\"/home/developing_nacho/fhdw/knowledge_engineering/fakenews_detection/models/distilbert-base-uncased/checkpoint-3455/rng_state.pth\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
